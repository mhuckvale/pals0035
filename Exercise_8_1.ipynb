{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_8_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_8_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsuMCkqDNArY"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 8.1\n",
        "\n",
        "In this exercise we use a seq2seq model to address the problem of grapheme to phoneme conversion. We use an English pronunciation dictionary called BEEP which has been converted to the [SAMPA machine readable prounciation](https://www.phon.ucl.ac.uk/home/sampa/) format. \n",
        "\n",
        "The training data looks like this:\n",
        "<ul><pre>\n",
        "PRONOUN\t      pr@UnaUn\n",
        "PRONOUN'S\t    pr@UnaUnz\n",
        "PRONOUNCE\t    pr@naUns\n",
        "PRONOUNCEABLE\tpr@naUns@bl\n",
        "</pre></ul>\n",
        "\n",
        "Our model will use an encoder RNN to generate a vector representing the spelling, then use a decoder RNN to unpack the vector into the pronunciation.\n",
        "Both encoder and decoder are trained together, which makes for a rather complex model in Keras, unlike previous exercises where we have used the Keras Sequential class to build the RNN.\n",
        "\n",
        "This exercise draws on the Keras example for seq2seq processing: [https://keras.io/examples/lstm_seq2seq/](https://keras.io/examples/lstm_seq2seq/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFuZqgM5IPD0"
      },
      "source": [
        "(a) Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f2ZGjXiIHPC"
      },
      "source": [
        "# Set up libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRysYfVPIhM8"
      },
      "source": [
        "---\n",
        "(b) Load the pronunciation dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0h0NxZCIizk"
      },
      "source": [
        "# the dictionary has pairs of input spelling and target transcription\n",
        "url=\"https://www.phon.ucl.ac.uk/courses/pals0039/data/english-dictionary.csv\"\n",
        "\n",
        "# read the dictionary using Pandas\n",
        "df=pd.read_csv(url,keep_default_na=False)\n",
        "\n",
        "# print the head and tail of the dictionary\n",
        "print(df.head())\n",
        "print(df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y6rx4SnOjjC"
      },
      "source": [
        "---\n",
        "(c) Build the input and output vocabulary of symbols."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1LIqtgMOyve"
      },
      "source": [
        "# get list of all characters used in orthography\n",
        "inputs=df.INPUT.to_list()\n",
        "input_chars=sorted(list(set(\"\".join(inputs))))\n",
        "\n",
        "# add space as a special character\n",
        "input_chars.append(' ')\n",
        "print(\"Spelling\",input_chars)\n",
        "\n",
        "# get list of all the characters used in transcription\n",
        "targets=df.TARGET.to_list()\n",
        "target_chars=sorted(list(set(\"\".join(targets))))\n",
        "\n",
        "# add space and square brackets as special characters - we will use these as start and end symbols\n",
        "target_chars.append('[')\n",
        "target_chars.append(']')\n",
        "target_chars.append(' ')\n",
        "print(\"Transcription\",target_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYwZ6YZggp1p"
      },
      "source": [
        "---\n",
        "(d) Get basic parameters for learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcpqs_PqgtFH"
      },
      "source": [
        "# number of pronunciations\n",
        "num_samples=len(df)\n",
        "\n",
        "# number of different tokens used in orthography\n",
        "num_encoder_tokens = len(input_chars)\n",
        "\n",
        "# number of different tokens used in transcription\n",
        "num_decoder_tokens = len(target_chars)\n",
        "\n",
        "# maximum lengths of words and pronunciations\n",
        "max_encoder_seq_length = max([len(txt) for txt in df.INPUT.to_list()])\n",
        "max_decoder_seq_length = max([len(txt) for txt in df.TARGET.to_list()])+2   # plus 2 for BOS=[ and EOS=]\n",
        "\n",
        "print('Number of samples:', num_samples)\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "# build dictionaries for letters and phones\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_chars)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_5Uoo-hj3yF"
      },
      "source": [
        "---\n",
        "(e) Build training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfmrCLpCj6l9"
      },
      "source": [
        "# list of spelling sequences to be used for training encoder\n",
        "encoder_input_data = np.zeros((num_samples, max_encoder_seq_length, num_encoder_tokens),dtype='float32')\n",
        "\n",
        "# list of transcription sequences to be used for training decoder (= target shifted right one place)\n",
        "decoder_input_data = np.zeros((num_samples, max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
        "\n",
        "# list of transcription sequences to be used as target for decoder output\n",
        "decoder_target_data = np.zeros((num_samples, max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
        "\n",
        "# encode orthogaphy as sequence of one-hot vectors\n",
        "def encode_str(str):\n",
        "  # pad input with spaces\n",
        "  input_text=str.ljust(max_encoder_seq_length)\n",
        "  # one hot coding\n",
        "  data=np.zeros((max_encoder_seq_length,num_encoder_tokens))\n",
        "  for t, char in enumerate(input_text):\n",
        "    data[t, input_token_index[char]] = 1.\n",
        "  return data\n",
        "\n",
        "# encode all pronunciations as sequences of one-hot vectors\n",
        "for i in range(num_samples):\n",
        "  # encode spelling\n",
        "  encoder_input_data[i,:,:]=encode_str(df.INPUT.at[i])\n",
        "  # encode target with BOS and EOS symbols\n",
        "  target_text=(\"[\"+df.TARGET.at[i]+\"]\").ljust(max_decoder_seq_length)\n",
        "  # set up deocder input and output\n",
        "  for t, char in enumerate(target_text):\n",
        "    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "    decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "    if t > 0:\n",
        "      # decoder_target_data will be ahead by one timestep\n",
        "      # and will not include the start character.\n",
        "      decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "# random shuffle the data\n",
        "nseq=encoder_input_data.shape[0]\n",
        "p = np.random.permutation(nseq)\n",
        "encoder_input_data = encoder_input_data[p]\n",
        "decoder_input_data = decoder_input_data[p]\n",
        "decoder_target_data = decoder_target_data[p]\n",
        "\n",
        "print(\"Encoder input\",encoder_input_data.shape,\"Decoder input\",decoder_input_data.shape,\"Decoder output\",decoder_target_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKgWFBu5n1eD"
      },
      "source": [
        "---\n",
        "(f) Build the seq2seq model for training. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjir9alan-OC"
      },
      "source": [
        "# Latent dimensionality of the encoding vector\n",
        "latent_dim = 256  \n",
        "\n",
        "# Define the input sequence and process it through LSTMs to get encoder state at end\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states as our embedding vector\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "# after the LSTM we use a dense layer to compute a PDF over output symbols\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# encoder_input_data & decoder_input_data  into decoder_target_data\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQV97IAodLJ"
      },
      "source": [
        "---\n",
        "(g) Train the seq2seq encoder. Training takes about 5 minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtderxQyoe6B"
      },
      "source": [
        "epochs = 5  # Number of epochs to train for.\n",
        "batch_size = 64  # Batch size for training.\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.05)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk0fHgSgo0AR"
      },
      "source": [
        "---\n",
        "(h) Build the inference model for transcribing new words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfcI9aFNo39p"
      },
      "source": [
        "# Next: inference mode (sampling).\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state and a \"start of sequence\" token as input.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with this target as input and the current state passed forward\n",
        "\n",
        "# Define sampling model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# take state from encoder\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# decoder LSTM takes decoder state and decoder input to produce one new output\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# function to run the encoder and decoder of the trained network on new input\n",
        "def decode_sequence(input_seq):\n",
        "  # reshape the input into a single row tensor\n",
        "  input_tensor=np.reshape(input_seq,(1,input_seq.shape[0],input_seq.shape[1]))\n",
        "  # Encode the input as a state vector\n",
        "  states_value = encoder_model.predict(input_tensor)\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  target_seq[0, 0, target_token_index['[']] = 1.\n",
        "\n",
        "  # Sampling loop for a batch of sequences\n",
        "  # (to simplify, here we assume a batch of size 1).\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "    # Sample a token (just choose most probable)\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "    # Exit condition: either hit max length\n",
        "    # or find stop character.\n",
        "    if (sampled_char == ']' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        "    else:\n",
        "      decoded_sentence += sampled_char\n",
        "\n",
        "    # Update the target sequence (of length 1).\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "    # Update states\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLA6BUdSpkVj"
      },
      "source": [
        "---\n",
        "(i) Try out model with some samples. Think of some words to test the pronunciation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR4LdI8NpoDx"
      },
      "source": [
        "examples=[ \"england\", \"wales\", \"scotland\", \"oxford\", \"glasshouse\"]\n",
        "for ex in examples:\n",
        "  data=encode_str(ex.upper())\n",
        "  decoded=decode_sequence(data)\n",
        "  print('-')\n",
        "  print('Input:', ex)\n",
        "  print('Pronunciation:', decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuOrp1T_2Pl9"
      },
      "source": [
        "while True:\n",
        "  ex=input(\"Type word (q to quit): \")\n",
        "  if ex==\"q\":\n",
        "    break\n",
        "  data=encode_str(ex.upper())\n",
        "  decoded=decode_sequence(data)\n",
        "  print('Input:', ex)\n",
        "  print('Pronunciation:', decoded)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}