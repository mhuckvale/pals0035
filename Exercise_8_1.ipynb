{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_8_1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_8_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsuMCkqDNArY",
        "colab_type": "text"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 8.1\n",
        "\n",
        "In this exercise we use a seq2seq model to address the problem of grapheme to phoneme conversion. We use an English pronunciation dictionary called BEEP which has been converted to the SAMPA machine readable prounciation format. This exercise draws on the Keras example for seq2seq processing: [https://keras.io/examples/lstm_seq2seq/](https://keras.io/examples/lstm_seq2seq/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFuZqgM5IPD0",
        "colab_type": "text"
      },
      "source": [
        "(a) Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f2ZGjXiIHPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRysYfVPIhM8",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(b) Load the pronunciation dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0h0NxZCIizk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url=\"https://www.phon.ucl.ac.uk/courses/pals0039/data/english-dictionary.csv\"\n",
        "\n",
        "df=pd.read_csv(url,keep_default_na=False)\n",
        "print(df.head())\n",
        "print(df.tail())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y6rx4SnOjjC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(c) Build the input and output vocabulary of symbols. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1LIqtgMOyve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "inputs=df.INPUT.to_list()\n",
        "input_chars=sorted(list(set(\"\".join(inputs))))\n",
        "# a\n",
        "input_chars.append(' ')\n",
        "print(input_chars)\n",
        "# \n",
        "targets=df.TARGET.to_list()\n",
        "target_chars=sorted(list(set(\"\".join(targets))))\n",
        "# \n",
        "target_chars.append('[')\n",
        "target_chars.append(']')\n",
        "target_chars.append(' ')\n",
        "print(target_chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYwZ6YZggp1p",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(d) Get basic parameters for learning. Run the code and add comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcpqs_PqgtFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "num_samples=len(df)\n",
        "# \n",
        "num_encoder_tokens = len(input_chars)\n",
        "# \n",
        "num_decoder_tokens = len(target_chars)\n",
        "# \n",
        "max_encoder_seq_length = max([len(txt) for txt in df.INPUT.to_list()])\n",
        "max_decoder_seq_length = max([len(txt) for txt in df.TARGET.to_list()])+2   # plus 2 for BOS=[ and EOS=]\n",
        "\n",
        "print('Number of samples:', num_samples)\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "# \n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_chars)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_5Uoo-hj3yF",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(e) Build training data. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfmrCLpCj6l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "encoder_input_data = np.zeros((num_samples, max_encoder_seq_length, num_encoder_tokens),dtype='float32')\n",
        "# \n",
        "decoder_input_data = np.zeros((num_samples, max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
        "# \n",
        "decoder_target_data = np.zeros((num_samples, max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
        "\n",
        "# \n",
        "def encode_str(str):\n",
        "  input_text=str.ljust(max_encoder_seq_length)\n",
        "  data=np.zeros((max_encoder_seq_length,num_encoder_tokens))\n",
        "  for t, char in enumerate(input_text):\n",
        "    data[t, input_token_index[char]] = 1.\n",
        "  return data\n",
        "\n",
        "# \n",
        "for i in range(num_samples):\n",
        "  encoder_input_data[i,:,:]=encode_str(df.INPUT.at[i])\n",
        "  target_text=(\"[\"+df.TARGET.at[i]+\"]\").ljust(max_decoder_seq_length)\n",
        "  for t, char in enumerate(target_text):\n",
        "    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "    decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "    if t > 0:\n",
        "      # decoder_target_data will be ahead by one timestep\n",
        "      # and will not include the start character.\n",
        "      decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "# \n",
        "nseq=encoder_input_data.shape[0]\n",
        "p = np.random.permutation(nseq)\n",
        "encoder_input_data = encoder_input_data[p]\n",
        "decoder_input_data = decoder_input_data[p]\n",
        "decoder_target_data = decoder_target_data[p]\n",
        "\n",
        "print(encoder_input_data.shape,decoder_input_data.shape,decoder_target_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKgWFBu5n1eD",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(f) Build the seq2seq model for training. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjir9alan-OC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "latent_dim = 256  \n",
        "\n",
        "# \n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# \n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# \n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# \n",
        "# \n",
        "# \n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# \n",
        "# \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# \n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQV97IAodLJ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(g) Train the seq2seq encoder. Training takes about 5 minutes. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtderxQyoe6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5  # \n",
        "batch_size = 64  # \n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.05)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk0fHgSgo0AR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(h) Build inference model. Run the code and add comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfcI9aFNo39p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next: inference mode (sampling).\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state and a \"start of sequence\" token as input.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with this target as input and the current state passed forward\n",
        "\n",
        "# \n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# \n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# \n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# \n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # \n",
        "  input_tensor=np.reshape(input_seq,(1,input_seq.shape[0],input_seq.shape[1]))\n",
        "  # \n",
        "  states_value = encoder_model.predict(input_tensor)\n",
        "  # \n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  # \n",
        "  target_seq[0, 0, target_token_index['[']] = 1.\n",
        "\n",
        "  # \n",
        "  # \n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "    # \n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "    # \n",
        "    # \n",
        "    if (sampled_char == ']' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        "    else:\n",
        "      decoded_sentence += sampled_char\n",
        "\n",
        "    # \n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "    # \n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLA6BUdSpkVj",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(i) Try out model with some samples. Think of some words to test the pronunciation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR4LdI8NpoDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples=[ \"england\", \"wales\", \"scotland\", \"oxford\", \"greenhouse\"]\n",
        "for ex in examples:\n",
        "  data=encode_str(ex.upper())\n",
        "  decoded=decode_sequence(data)\n",
        "  print('-')\n",
        "  print('Input:', ex)\n",
        "  print('Pronunciation:', decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuOrp1T_2Pl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while True:\n",
        "  ex=input(\"Type word (q to quit): \")\n",
        "  if ex==\"q\":\n",
        "    break\n",
        "  data=encode_str(ex.upper())\n",
        "  decoded=decode_sequence(data)\n",
        "  print('Input:', ex)\n",
        "  print('Pronunciation:', decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}