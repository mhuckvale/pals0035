{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_6_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_6_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is1p5hg4EqlY"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 6.1\n",
        "\n",
        "In this exercise we revisit the IMDB sentiment analysis task from exercise 4.1, but this time exploiting Glove word embeddings and a recurrent network for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GSGA1ZvEwTp"
      },
      "source": [
        "(a) Load the IMDB review data set. Run the code and add comments,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuC220KlEni4"
      },
      "source": [
        "# \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# \n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, SimpleRNN, LSTM, GRU\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# \n",
        "(Xtrain_seq,ytrain_seq),(Xtest_seq,ytest_seq)=imdb.load_data(num_words=10000)\n",
        "\n",
        "#\n",
        "print(Xtrain_seq[:5])\n",
        "print(ytrain_seq[:5])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYyhzqKZMPqa"
      },
      "source": [
        "---\n",
        "(b) Print some basic parameters of the data set and an example review. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NueQeJgtMP6y"
      },
      "source": [
        "# \n",
        "listlengths=[]\n",
        "for sequence in Xtrain_seq:\n",
        "  listlengths.append(len(sequence))\n",
        "print(\"Longest list\",max(listlengths))\n",
        "\n",
        "# \n",
        "wordcodes=[]\n",
        "for sequence in Xtrain_seq:\n",
        "  wordcodes.append(max(sequence))\n",
        "print(\"Highest code\",max(wordcodes))\n",
        "\n",
        "# \n",
        "dictionary=imdb.get_word_index();\n",
        "print(\"the\",dictionary['the'])\n",
        "reverse_dictionary={0:\"padding\",1:\"BOS\",2:\"UNK\"}\n",
        "for (key,value) in dictionary.items():\n",
        "  reverse_dictionary[value+3]=key\n",
        "\n",
        "# \n",
        "print(list(map(reverse_dictionary.get, range(10))))\n",
        "\n",
        "# \n",
        "print(\"First review\")\n",
        "review=[]\n",
        "for i in range(len(Xtrain_seq[0])):\n",
        "  review.append(reverse_dictionary[Xtrain_seq[0][i]])\n",
        "print(\" \".join(review))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uO_gR0wFS3S"
      },
      "source": [
        "---\n",
        "(c) Load the Glove embeddings. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOqqEYhOFZNg"
      },
      "source": [
        "# \n",
        "df=pd.read_csv('https://www.phon.ucl.ac.uk/courses/pals0039/data/glove.6B.100d.zip',header=None)\n",
        "df.rename(columns={0:\"word\"},inplace=True)\n",
        "print(\"Read %d word embeddings of length %d\" % (len(df),len(df.columns)-1))\n",
        "embedsize=len(df.columns)-1\n",
        "\n",
        "# \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVcibJjGFnfx"
      },
      "source": [
        "---\n",
        "(d) Build a look up index for Glove. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwXiAjGYFpz9"
      },
      "source": [
        "# \n",
        "glove_index={}\n",
        "for i,word in enumerate(df.word):\n",
        "  glove_index[word]=i\n",
        "\n",
        "# \n",
        "print(\"#words\",len(glove_index))\n",
        "print(glove_index['the'],glove_index['white'],glove_index['cat'])\n",
        "print(df.word[glove_index['the']],df.word[glove_index['white']],df.word[glove_index['cat']])\n",
        "\n",
        "# \n",
        "glove_embed=np.array(df.iloc[:,1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DkHwy6xLv1i"
      },
      "source": [
        "---\n",
        "(e) map the IMDB vocabulary to the Glove dictionary. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjvXismELz75"
      },
      "source": [
        "# \n",
        "word_embed=np.zeros((10000,embedsize))\n",
        "# \n",
        "for i in range(10000):\n",
        "  if i <= 3:\n",
        "    word='.'\n",
        "  elif i in reverse_dictionary:\n",
        "    word=reverse_dictionary[i]\n",
        "  else:\n",
        "    print(\"Word index %d not found\" % (i))\n",
        "  if word.lower() in glove_index:\n",
        "    eidx=glove_index[word.lower()]\n",
        "  else:\n",
        "    eidx=glove_index['.']\n",
        "  word_embed[i,:]=glove_embed[eidx,:]\n",
        "\n",
        "# \n",
        "idx_the=glove_index['the']\n",
        "emb_the=glove_embed[idx_the]\n",
        "widx_the=dictionary['the']+3\n",
        "wemb_the=word_embed[widx_the]\n",
        "print(idx_the,emb_the)\n",
        "print(widx_the,wemb_the);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpD7dkujGISh"
      },
      "source": [
        "---\n",
        "(e) Prepare IMDB sequences to fixed length padded at start. Run code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd6iHP6MGNMi"
      },
      "source": [
        "# \n",
        "maxseq=500\n",
        "\n",
        "# \n",
        "Xtrain_pad=pad_sequences(Xtrain_seq,maxlen=maxseq,padding='pre',value=0)\n",
        "Xtest_pad=pad_sequences(Xtest_seq,maxlen=maxseq,padding='pre',value=0)\n",
        "\n",
        "# \n",
        "print(Xtrain_pad.shape)\n",
        "print(Xtrain_pad[:5,])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABgvXFckOnjB"
      },
      "source": [
        "---\n",
        "(f) Build a model with embeddings in first layer. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvoA4IvbOpen"
      },
      "source": [
        "# \n",
        "model = Sequential()\n",
        "# \n",
        "model.add(Embedding(10000, embedsize, weights=[word_embed], input_length=maxseq, trainable=False))\n",
        "model.add(LSTM(32,return_sequences=False,dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# \n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# \n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0NtDvjscfmK"
      },
      "source": [
        "---\n",
        "(g) Train the model. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO1kE_z6chK2"
      },
      "source": [
        "# \n",
        "history=model.fit(Xtrain_pad, ytrain_seq, epochs=15, verbose=1,batch_size=64,validation_split=0.05)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieqTwwWuQ6UX"
      },
      "source": [
        "---\n",
        "(h) Plot training graphs. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlWg9PXNQ8_F"
      },
      "source": [
        "# \n",
        "hist=history.history\n",
        "epochs=range(1,len(hist['loss'])+1)\n",
        "\n",
        "#\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs,hist['loss'],'bo',label=\"Training loss\")\n",
        "plt.plot(epochs,hist['val_loss'],'b-',label=\"Validation loss\")\n",
        "plt.title(\"Training and Validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# \n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs,hist['accuracy'],'bo',label=\"Training accuracy\")\n",
        "plt.plot(epochs,hist['val_accuracy'],'b-',label=\"Validation accuracy\")\n",
        "plt.title(\"Training and Validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMmYJkNaTSHN"
      },
      "source": [
        "---\n",
        "(i) Get test performance. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oiddz_GTTVKD"
      },
      "source": [
        "# \n",
        "score, acc = model.evaluate(Xtest_pad, ytest_seq,verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "# \n",
        "ypred=model.predict(Xtest_pad)\n",
        "ypred=ypred.flatten()\n",
        "print(list(zip(ytest_seq[:10],ypred[:10])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdsHx5-Dj538"
      },
      "source": [
        "---\n",
        "(j) Experiment with the network architecture and training protocol for this problem. For example you might try:\n",
        "<ul>\n",
        "<li>Changing the number of LSTM nodes\n",
        "<li>Changing the activation function in the LSTM\n",
        "<li>Adding a second LSTM layer\n",
        "<li>Changing to a bi-directional LSTM layer.\n",
        "</ul>\n",
        "\n",
        "What is the best result you can get on the test set?"
      ]
    }
  ]
}