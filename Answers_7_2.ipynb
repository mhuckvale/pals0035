{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_7_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_7_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7KKVn4ADsTj"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 7.2\n",
        "\n",
        "In this exercise we build a word language model using a recurrent network. We test it on a Cloze task in which we have to choose which word fits best within a given sentence.\n",
        "\n",
        "For training the language model we use a collection of copyright free stories from the Gutenberg arhive.\n",
        "\n",
        "The Cloze test comes from the [Children's Book Test](https://arxiv.org/pdf/1511.02301.pdf). In this test you are presented with an extract of a story and then a sentence with one word missing. Your task is to pick the right word to fill in the blank from the alternatives provided.\n",
        "\n",
        "Here is an example:\n",
        "<ul><br>CONTEXT: ... When the youth had overtaken them , he saw that there was a clear spring in the middle of the space . He sat down at the foot of the tree upon which the birds were perched , and listened attentively to what they were saying to each other . The sun is not down yet ,  said the first bird ;  we must wait yet awhile till the moon rises and the maiden comes to the spring .\t<br>QUERY: Do you think she will see that young\t____ sitting under the tree ?\n",
        "<br>ALTERNATIVES:\tbirds|food|maiden|man|middle|place|sight|south|wash|wings\n",
        "<br>CORRECT: man\n",
        "</ul>\n",
        "\n",
        "Our approach is to build a language model from a set of children's stories, using sequences of length 100. We then use the end of the CONTEXT and the QUERY text up to the blank as the starting point for predicting the blank (we don't use the right part of the QUERY at all) which provides a distribution over the vocabulary. We then choose the most probable word from the alternatives. This is certainly not an optimum approach and there are many better ways of solving this problem. The best approaches can achieve over 75% on this CBTest/CN task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAGoWqZaDuLi"
      },
      "source": [
        "(a) Setup libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK3T32_LDmD2"
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, SimpleRNN, LSTM, GRU, Bidirectional, Dropout, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import get_file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_JMdV-wDzaD"
      },
      "source": [
        "---\n",
        "(b) Load the text we will use for training. This is a collection of stories from the Gutenberg archive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWFm0EEpD2W7"
      },
      "source": [
        "# download the training text\n",
        "url = \"https://www.phon.ucl.ac.uk/courses/pals0039/data/cloze-corpus.txt\"\n",
        "response = requests.get(url)\n",
        "# convert the text to a single lower case string\n",
        "raw_text = response.text.lower().replace('\\n',' ')\n",
        "print(\"Corpus has\",len(raw_text),\"characters\")\n",
        "print(raw_text[:250])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxz2xmaREHuz"
      },
      "source": [
        "---\n",
        "(c) Tokenize the text. Then build a vocabulary of 10,000 words and set the rare words to unknown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la_koZiXESba"
      },
      "source": [
        "# set the vocabulary limit to be 10,000 words\n",
        "max_words=10000\n",
        "\n",
        "# use the Keras Tokenizer\n",
        "tokenizer = Tokenizer(num_words=max_words,oov_token=\"UNK\")\n",
        "tokenizer.fit_on_texts([raw_text])\n",
        "word_index=tokenizer.word_index\n",
        "print(\"Found\",len(word_index),\"different words.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouxB9goEFqaq"
      },
      "source": [
        "# print the 10 most common and 10 least common words\n",
        "print(list(word_index.items())[:10])\n",
        "print(list(word_index.items())[-10:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XK6h7FyGyMJ"
      },
      "source": [
        "# convert the text to a list of word indexes, replacing rare words with UNK\n",
        "raw_seq=tokenizer.texts_to_sequences([raw_text])[0]\n",
        "# print the start of the sample\n",
        "print(raw_seq[:50])\n",
        "# report largest code\n",
        "print(\"# words\",len(raw_seq),\"Max index\",max(raw_seq))\n",
        "# calculate the out-of-vocabulary rate\n",
        "num_oov=sum(1 for w in raw_seq if w==1)\n",
        "print(\"%OOV\",100*num_oov/len(raw_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a8iMhEgJJST"
      },
      "source": [
        "---\n",
        "(d) Prepare the text sample for training by dividing into sequences of 100 words. Then save some of the data for validation of the language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVk3QwLrJLiE"
      },
      "source": [
        "# divide the text into sequences of fixed length\n",
        "seq_len=100\n",
        "nseq=len(raw_seq)//seq_len\n",
        "# chunk the text into sequences\n",
        "seq=np.reshape(raw_seq[:nseq*seq_len],(nseq,seq_len))\n",
        "# shift the text back one word\n",
        "raw_seq_shift=np.roll(raw_seq,-1) \n",
        "# and chunk into sequences to act as targets\n",
        "seq_shift=np.reshape(raw_seq_shift[:nseq*seq_len],(nseq,seq_len))\n",
        "# randomise the order of the sequences\n",
        "p = np.random.permutation(nseq)\n",
        "seq=seq[p]\n",
        "seq_shift=seq_shift[p];\n",
        "\n",
        "# divide into train and validation sets\n",
        "nval=nseq//20\n",
        "Xval=seq[:nval,:]\n",
        "yval=seq_shift[:nval,:]\n",
        "Xtrain=seq[nval:,:]\n",
        "ytrain=seq_shift[nval:,:]\n",
        "\n",
        "print(\"Train size\",Xtrain.shape,ytrain.shape)\n",
        "print(\"Validation size\",Xval.shape,yval.shape)\n",
        "\n",
        "print(Xtrain[0,:10],ytrain[0,:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIc0HRVTI2cu"
      },
      "source": [
        "---\n",
        "(e) Build a recurrent language model with two LSTM layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXfWunrUI5R8"
      },
      "source": [
        "import tensorflow as tf\n",
        "# define a function for Keras to report perplexity during training\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = tf.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "    return perplexity\n",
        "\n",
        "# set up basic sizes for nextwork\n",
        "isize=max_words\n",
        "embed_size=64\n",
        "osize=max_words\n",
        "\n",
        "# build the model with an embedding layer and two layers of LSTMs\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=isize, output_dim=embed_size,input_length=seq_len))\n",
        "model.add(LSTM(32,return_sequences=True,activation='tanh'))\n",
        "model.add(LSTM(32,return_sequences=True,activation='tanh'))\n",
        "model.add(TimeDistributed(Dense(osize, activation='softmax')));\n",
        "#\n",
        "# compile the network\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=[perplexity])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxlpOwNCMjUU"
      },
      "source": [
        "---\n",
        "(f) Train the model. There is also optional code here to save and restore trained models since training can take some time (about 30min with GPU)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9R6egn-Mk9D"
      },
      "source": [
        "# train the model\n",
        "history=model.fit(Xtrain,ytrain, batch_size=64, validation_data=(Xval,yval), epochs=25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8M1XwKoRhm0"
      },
      "source": [
        "# (optional) save the model to your Google drive account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_save_name = 'ex72.h5'\n",
        "path = \"/content/gdrive/My Drive/\"+model_save_name\n",
        "model.save(path,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4EBOPi7TKih"
      },
      "source": [
        "# (optional) load a trained model from your Google drive account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_save_name = 'ex72.h5'\n",
        "path = \"/content/gdrive/My Drive/\"+model_save_name\n",
        "model=load_model(path, custom_objects={'perplexity': perplexity})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4il7VQlTmoh"
      },
      "source": [
        "# (optional) load a trained model from the course website\n",
        "model_save_name = 'ex72.h5'\n",
        "url = \"https://www.phon.ucl.ac.uk/courses/pals0039/data/\"+model_save_name\n",
        "file = get_file(model_save_name,url,cache_subdir=\"models\")\n",
        "model=load_model(file, custom_objects={'perplexity': perplexity})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVJIa66JVuLA"
      },
      "source": [
        "---\n",
        "(g) Load the data for the cloze task to use for testing. The cloze test data is stored in a spreadsheet with named columns for the relevant parts of each test question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgt_wesuBIGu"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read in the Cloze data set from the course web site.\n",
        "df=pd.read_csv(\"https://www.phon.ucl.ac.uk/courses/pals0039/data/cloze-test.csv\",keep_default_na=False)\n",
        "\n",
        "# the basics of the Cloze task is made up of the columns: df.CONTEXT, df.QUERY and df.ALTERNATIVES with the correct answer in df.ANSWER\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttBq6-LvY9EX"
      },
      "source": [
        "---\n",
        "(h) Encode the cloze test data using the tokenizer and assemble into sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdVmtbtUZjm1"
      },
      "source": [
        "# concatenate the context and the query and divide up the alternatives\n",
        "cloze_context=[]\n",
        "cloze_answer=[]\n",
        "cloze_alter=[]\n",
        "for i in range(len(df)):\n",
        "  # concatenate the CONTEXT and the QUERY to get sufficient text for the LM\n",
        "  str=df.CONTEXT.iat[i]+\" \"+df.CONTEXT.iat[i]+\" \"+df.QUERY.iat[i]\n",
        "  cloze_context.append(str)\n",
        "  cloze_answer.append(df.ANSWER.iat[i])\n",
        "  cloze_alter.append((df.ALTERNATIVES.iat[i]).split('|'))\n",
        "\n",
        "# convert the strings to integer sequences\n",
        "cloze_context_seq=tokenizer.texts_to_sequences(cloze_context)\n",
        "cloze_answer_seq=tokenizer.texts_to_sequences(cloze_answer)\n",
        "cloze_alter_seq=tokenizer.texts_to_sequences(cloze_alter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv7WufBtbmtc"
      },
      "source": [
        "# print some samples of what we have\n",
        "print(cloze_context[0])\n",
        "print(cloze_context_seq[0])\n",
        "print(cloze_answer[:10])\n",
        "print(cloze_answer_seq[:10])\n",
        "print(cloze_alter[:10])\n",
        "print(cloze_alter_seq[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNYhwMwMb_xk"
      },
      "source": [
        "---\n",
        "(i) Run the model over the all the test sequences and obtain a pdf over the word that completes the query. We first chop down the context to the last 100 words to fit the sequence length used to train the lanugage model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr7fn3tXcGtP"
      },
      "source": [
        "seq_len=100\n",
        "# chop all context sequences down to seq_len values by taking the last 100 words\n",
        "cloze_context_lim=np.stack(np.array([ x[-seq_len:] for x in cloze_context_seq]))\n",
        "print(cloze_context_lim.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0de5QDqAawi"
      },
      "source": [
        "# run the model in batches of 50 so as not to overload memory in Colab\n",
        "block_size=50\n",
        "nblock=cloze_context_lim.shape[0]//block_size\n",
        "# array to hold the word probabilities\n",
        "ypred=np.zeros((nblock*block_size,max_words))\n",
        "for i in range(nblock):\n",
        "  # get the predicted word probabilities\n",
        "  testdata=cloze_context_lim[i*block_size:(i+1)*block_size,:]\n",
        "  pred=model.predict(testdata,batch_size=50)\n",
        "  # save the probabilities for the last predicted word\n",
        "  ypred[i*block_size:(i+1)*block_size,:]=pred[:,-1,:]\n",
        "\n",
        "print(ypred.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT5RBA8vVaQK"
      },
      "source": [
        "---\n",
        "(j) For each cloze sentence, find the probabilities of each of the alternatives and choose the most probable. Finally compare our answer with the ANSWER field and keep a record of how well we did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyjxZfMjFBDl"
      },
      "source": [
        "# for each cloze sentence in turn\n",
        "ntest=ypred.shape[0]\n",
        "ncorrect=0;\n",
        "for i in range(ntest):\n",
        "  # get the number of alternatives\n",
        "  nprob=len(cloze_alter_seq[i])\n",
        "  # get the predicted probabilities\n",
        "  prob=np.zeros(nprob)\n",
        "  for j in range(nprob):\n",
        "    prob[j]=ypred[i,cloze_alter_seq[i][j]]\n",
        "  # choose the most probable among the alternatives\n",
        "  top_word=cloze_alter_seq[i][np.argmax(prob)]\n",
        "  # get the actually correct word\n",
        "  correct_word=cloze_answer_seq[i][0]\n",
        "  # record whether we got it right\n",
        "  if (top_word==correct_word):\n",
        "    ncorrect += 1\n",
        "\n",
        "# print how well we did\n",
        "print(\"Correct: %d/%d (%.1f%%)\" % (ncorrect,ntest,100*ncorrect/ntest))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyZ7iU63M5ax"
      },
      "source": [
        "(k) Experiment with the network architecture and training protocol to see if you can improve performance on the Cloze task. Advanced: find a way to use the right-hand part of the QUERY string. One way coule be to evaluate each ALTERNATIVE in turn embedded in the QUERY sentence and choose the one which is given the greatest probability by the language model."
      ]
    }
  ]
}