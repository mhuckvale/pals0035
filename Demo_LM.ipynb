{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_LM.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbhJYdxpi/pLJ8X30dYrs+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Demo_LM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QYlAUr-y-kX",
        "colab_type": "text"
      },
      "source": [
        "#Language modelling demos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4-wXWvRPUcX",
        "colab_type": "text"
      },
      "source": [
        "##Use NLTK Language Modelling functions for building and testing an n-gram language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWAREcBkQU4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk==3.4.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znfr9EdrPcoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "nltk.download('punkt')\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.lm.preprocessing import flatten\n",
        "\n",
        "text=[]\n",
        "for s in reuters.sents():\n",
        "  slower=map(str.lower,s)\n",
        "  text.append(list(pad_sequence(slower,pad_left=True,left_pad_symbol=\"<s>\",pad_right=True,right_pad_symbol=\"</s>\",n=2)))\n",
        "\n",
        "for i in range(5):\n",
        "  print(text[i])\n",
        "\n",
        "text=list(flatten(text))\n",
        "print('Text length',len(text))\n",
        "vocab=Vocabulary(text, unk_cutoff=10)\n",
        "print('Vocab size',len(vocab))\n",
        "\n",
        "text=vocab.lookup(text)\n",
        "\n",
        "print(text[:25])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy5t68a-FQ2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# collect and display ngram counts\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm import NgramCounter\n",
        "allgrams=list(everygrams(text,min_len=1,max_len=3))\n",
        "print(len(text),len(allgrams))\n",
        "ngram_counts = NgramCounter([allgrams])\n",
        "\n",
        "# display some continuations\n",
        "print(sorted(ngram_counts[('imports',)].items(),key = lambda x: x[1],reverse=True)[:10])\n",
        "print(sorted(ngram_counts[('imports','of')].items(),key = lambda x: x[1],reverse=True)[:10])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P41EB6UFd3Us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.lm.models import Laplace\n",
        "from nltk.util import ngrams\n",
        "for order in (1,2,3):\n",
        "  train=list(ngrams(text[10000:],order))\n",
        "  test=list(ngrams(text[:10000],order))\n",
        "  print(train[:10])\n",
        "  lm = Laplace(order)\n",
        "  lm.fit([train],vocab)\n",
        "  print(\"Train perplexity\",lm.perplexity(train[:10000]))\n",
        "  print(\"Test perplexity\",lm.perplexity(test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTSiakBf0V0h",
        "colab_type": "text"
      },
      "source": [
        "## neural model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ZCTBVR0Uph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, LSTM, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLTiMTwM0Y2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a vocabulary\n",
        "lexicon={}\n",
        "for w in text:\n",
        "  if not w in lexicon:\n",
        "    lexicon[w]=len(lexicon)\n",
        "\n",
        "# encode text as numbers\n",
        "ntext=[lexicon[w] for w in text]\n",
        "print(text[:25])\n",
        "print(ntext[:25])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOlq9lmbAh4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequences(text,seqlen):\n",
        "  nseq=(len(text)-1)//seqlen\n",
        "  feats=np.zeros((nseq,seqlen))\n",
        "  labels=np.zeros((nseq,seqlen))\n",
        "  for i in range(nseq):\n",
        "    feats[i,:]=text[i*seqlen:i*seqlen+seqlen]       # input is text sequence\n",
        "    labels[i,:]=text[i*seqlen+1:i*seqlen+seqlen+1]  # output is text sequence advanced by 1\n",
        "  return feats,labels\n",
        "\n",
        "seqlen=100\n",
        "Xtrain,ytrain = prepare_sequences(ntext[10000:],seqlen)\n",
        "Xtest,ytest = prepare_sequences(ntext[:10000],seqlen)\n",
        "\n",
        "print(Xtrain.shape,ytrain.shape)\n",
        "print(Xtest.shape,ytest.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNA3_LpIC-CO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = tf.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "isize=len(lexicon)\n",
        "osize=len(lexicon)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=isize, output_dim=64,input_length=seqlen))\n",
        "model.add(LSTM(32,return_sequences=True,activation='tanh'))\n",
        "model.add(LSTM(32,return_sequences=True,activation='tanh'))\n",
        "model.add(TimeDistributed(Dense(osize, activation='softmax')));\n",
        "#\n",
        "# compile the network\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=[perplexity])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJPQRO2mDiZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "history=model.fit(Xtrain,ytrain, batch_size=64, validation_data=(Xtest,ytest), epochs=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc_NDveSaOP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate from trained model\n",
        "\n",
        "# build a reverse dictionary\n",
        "reverse_lexicon={}\n",
        "for k,v in lexicon.items():\n",
        "  reverse_lexicon[v]=k\n",
        "\n",
        "# sample from a probability distribution\n",
        "def sampledist(dist):\n",
        "  thresh=np.random.random()\n",
        "  sum=0\n",
        "  for i in range(len(dist)):\n",
        "    sum += dist[i]\n",
        "    if sum > thresh:\n",
        "      return(i)\n",
        "  return(0)\n",
        "\n",
        "# pick a starting seed\n",
        "inptext=\"<s> it is a fact that\"\n",
        "\n",
        "# encode as numbers\n",
        "pattern=[lexicon[vocab.lookup(w)] for w in inptext.split(' ')]\n",
        "pat_len=len(pattern)\n",
        "pattern=pad_sequences([pattern],maxlen=seqlen,padding='post')\n",
        "\n",
        "# generate words\n",
        "for i in range(seqlen-pat_len):\n",
        "  prediction = model.predict(pattern, verbose=0)\n",
        "  prediction = prediction[0]\n",
        "  pattern[0][pat_len]=sampledist(prediction[pat_len,:])\n",
        "  pat_len += 1\n",
        "\n",
        "sentence=[]\n",
        "for i in range(pat_len):\n",
        "  sentence.append(reverse_lexicon[pattern[0][i]])\n",
        "print(' '.join(sentence))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}