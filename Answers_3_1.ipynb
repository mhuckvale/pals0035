{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_3_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1dW6Y-FBfg5"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "#Exercise 3.1 Answers\n",
        "\n",
        "In this exercise we use gradient descent to train a one neuron regresison problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvI1FbQVLpX2"
      },
      "source": [
        "(a) The following code builds some data for testing a regression problem. Run the code, then add comments to explain the different steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZWU5iE2BfFi"
      },
      "source": [
        "# import libaries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# set up true slope and intercept for regression problem\n",
        "trueslope=4\n",
        "trueintercept=2\n",
        "N=100\n",
        "\n",
        "# generate the training data\n",
        "x=np.linspace(0,1,N)\n",
        "y=trueintercept+trueslope*x+np.random.normal(size=N)\n",
        "\n",
        "# plot the training data\n",
        "plt.plot(x,y,'bo')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CRvLrn_B5pQ"
      },
      "source": [
        "---\n",
        "(b) Define a loss function - explain what this code does"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VDM9s2CB-iA"
      },
      "source": [
        "def loss(ytrue,ypred):\n",
        "  # returns the root mean square prediction error\n",
        "  return np.sqrt(np.mean(np.square(np.subtract(ypred,ytrue))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-gmFZOgCEng"
      },
      "source": [
        "---\n",
        "(c) Set up a single neuron with input weight W and bias B - explain what this code does.\n",
        "Note that this neuron does not have an activation function - it is a \"linear\" unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drsbAMdyCQFw"
      },
      "source": [
        "# defines the operation of the neuron: B=Bias, W=Weights, x=input\n",
        "def neuron(W,B,x):\n",
        "  return B+W*x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVXdGuy2C-iE"
      },
      "source": [
        "---\n",
        "(d) Calculate gradients given weights and loss - explain what this code does"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7WGJlWODJxf"
      },
      "source": [
        "def gradients(x,y,ypred):\n",
        "  # average gradient of W w.r.t. error\n",
        "  deltaW=np.mean(-x*(ypred-y))\n",
        "  # average gradient of B w.r.t. error\n",
        "  deltaB=np.mean(-(ypred-y))\n",
        "  return deltaW,deltaB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCwvgp-zDfK_"
      },
      "source": [
        "---\n",
        "(e) Fit the model to the training data. Run the code and then add comments to explain its operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F-LQ5eUDlo3"
      },
      "source": [
        "# set the initial weight and bias to small random values\n",
        "W=np.random.normal(0.0,0.1)\n",
        "B=np.random.normal(0.0,0.1)\n",
        "\n",
        "# specify the learning rate\n",
        "learnrate=0.5\n",
        "\n",
        "# for 100 training epochs\n",
        "for epoch in range(1,101):\n",
        "  # get the current prediction for all input values\n",
        "  ypred=neuron(W,B,x)\n",
        "  # get the average loss for the current neuron\n",
        "  err=loss(y,ypred)\n",
        "  # print progress\n",
        "  print(epoch,err)\n",
        "  # calculate the changes in the weights\n",
        "  deltaW,deltaB=gradients(x,y,ypred)\n",
        "  # update the weights\n",
        "  W += learnrate*deltaW\n",
        "  B += learnrate*deltaB\n",
        "\n",
        "# print the final weights\n",
        "print(W,B)\n",
        "\n",
        "# find the best possible regression line\n",
        "poly=np.polyfit(x,y,1)\n",
        "ypoly=np.polyval(poly,x)\n",
        "\n",
        "# plot the neuron predictions and the best fit\n",
        "plt.plot(x,y,'bo')\n",
        "plt.plot(x,ypred,'r-')\n",
        "plt.plot(x,ypoly,'g:')\n",
        "plt.title(\"neuron loss=%.3f, best loss = %.3f\" % (loss(y,ypred),loss(y,ypoly)))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}