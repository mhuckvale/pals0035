{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_Seq2Seq.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNn8mTX4cknu6pve4dGri08",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Demo_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6enwzy3hSvtH",
        "colab_type": "text"
      },
      "source": [
        "# Demonstration of Sequence-to-Sequence processing\n",
        "\n",
        "This demonstration adapted from [https://keras.io/examples/lstm_seq2seq/](https://keras.io/examples/lstm_seq2seq/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddEB7iIHSqXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPlG4QzCS5nG",
        "colab_type": "text"
      },
      "source": [
        "Synthesize some training data. The task is to take a string of alphabetic characters as input and to output the same list reversed in order and converted to upper case.  For example \"abcde\" -> \"EDCBA\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKIXfzdqS8yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input and output vocabularies\n",
        "IVOCAB={ 0:'#', 1:'a', 2:'b', 3:'c', 4:'d', 5:'e'}\n",
        "OVOCAB={ 0:'#', 1:'A', 2:'B', 3:'C', 4:'D', 5:'E'}\n",
        "\n",
        "# create some random sequences\n",
        "seqlen=8\n",
        "nseq=10000\n",
        "xdata=[]    # encoder input\n",
        "ydata=[]    # decoder output\n",
        "tdata=[]    # decoder input\n",
        "for i in range(nseq):\n",
        "  seq=[]\n",
        "  for j in range(seqlen):\n",
        "    seq.append(1+np.random.choice(len(IVOCAB)-1))\n",
        "  seq.append(0)\n",
        "  xdata.append(seq)\n",
        "  seq=seq[::-1]\n",
        "  ydata.append(seq[1:])\n",
        "  tdata.append(seq[:-1])\n",
        "\n",
        "# split into train and test\n",
        "ntrain=int(0.9*nseq)\n",
        "xtrain=xdata[:ntrain]\n",
        "ytrain=ydata[:ntrain]\n",
        "ttrain=tdata[:ntrain]\n",
        "xtest=xdata[ntrain:]\n",
        "ytest=ydata[ntrain:]\n",
        "ttest=tdata[ntrain:]\n",
        "\n",
        "def inputstr(seq):\n",
        "  seq=[IVOCAB[v] for v in seq]\n",
        "  return ''.join(seq)\n",
        "def outputstr(seq):\n",
        "  seq=[OVOCAB[v] for v in seq]\n",
        "  return ''.join(seq)\n",
        "\n",
        "def printseq(x,t,y):\n",
        "  print(\"Encoder-Input\",inputstr(x),\"Decoder-Input\",outputstr(t),\"Decoder-Output\",outputstr(y))\n",
        "\n",
        "for i in range(5):\n",
        "  printseq(xtrain[i],ttrain[i],ytrain[i])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMXMGFszVm_d",
        "colab_type": "text"
      },
      "source": [
        "Build an encoder-decoder pair for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab7uYmw_V1Qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "isize=len(IVOCAB)\n",
        "latent_dim=100\n",
        "osize=len(OVOCAB)\n",
        "\n",
        "# build encoder network\n",
        "encoder_inputs = Input(shape=(None, isize))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, osize))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(osize, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH1Ue4nLslmk",
        "colab_type": "text"
      },
      "source": [
        "Build the training data and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL--ybs1soQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert our sequences to one-hot coding\n",
        "encoder_input_data=to_categorical(xtrain)\n",
        "decoder_input_data=to_categorical(ttrain)\n",
        "decoder_target_data=to_categorical(ytrain)\n",
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)\n",
        "print(decoder_target_data.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VaOM90ItpRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 25  # Number of epochs to train for.\n",
        "batch_size = 64  # Batch size for training.\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al86HatTuW67",
        "colab_type": "text"
      },
      "source": [
        "Build a sequence encoder-decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2PXPID_ubB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # reshape input into tensor of size 1\n",
        "  input_tensor=input_seq.reshape(1,input_seq.shape[0],input_seq.shape[1])\n",
        "  # Encode the input as state vectors.\n",
        "  states_value = encoder_model.predict(input_tensor)\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1, osize))\n",
        "  # Populate the first element of target sequence with the start symbol #.\n",
        "  target_seq[0, 0, 0] = 1.\n",
        "\n",
        "  # Sampling loop for a batch of sequences\n",
        "  # (to simplify, here we assume a batch of size 1).\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "    # Sample a token\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = OVOCAB[sampled_token_index]\n",
        "\n",
        "    # Exit condition: either hit max length\n",
        "    # or find stop character.\n",
        "    if (sampled_char == '#' or len(decoded_sentence) >= seqlen):\n",
        "      stop_condition = True\n",
        "    else:\n",
        "      decoded_sentence += sampled_char\n",
        "\n",
        "    # Update the target sequence (of length 1).\n",
        "    target_seq = np.zeros((1, 1, osize))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "    # Update states\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYHi-5kuzAFf",
        "colab_type": "text"
      },
      "source": [
        "Test the encoder-decoder with some test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmJNY2V3u8mZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "encoder_input_test=to_categorical(xtest)\n",
        "\n",
        "for i in range(10):\n",
        "  inp=inputstr(xtest[i])\n",
        "  tar=outputstr(ytest[i])\n",
        "  out=decode_sequence(encoder_input_test[i])\n",
        "  print(\"Input\",inp,\"Target\",tar,\"Predicted\",out,\"OK\" if out==tar else \"Fail\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}