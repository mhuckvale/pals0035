{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_7_1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaxAfBk1iWFo",
        "colab_type": "text"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 7.1 Answers\n",
        "\n",
        "In this exercise we train a character sequence language model using a recurrent neural network and then generate some random text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKlBlwuGigPf",
        "colab_type": "text"
      },
      "source": [
        "(a) Import the standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NyvFYGmiVgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, SimpleRNN, LSTM, GRU, Bidirectional, Dropout, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQzyn_HwkS62",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(b) Read in text file and convert characters to a list of integers. Run the code and add comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq9BEcbFkXNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the text file of alice in wonderland\n",
        "url = \"https://www.phon.ucl.ac.uk/courses/pals0039/data/alice.txt\"\n",
        "response = requests.get(url)\n",
        "# convert everything to lower case and replace newlines with spaces.\n",
        "raw_text = response.text.lower().replace('\\n',' ')\n",
        "print(raw_text[:250])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWGzYeu_mYvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "# number of different characters\n",
        "NCHAR=len(chars)\n",
        "print(NCHAR)\n",
        "# convert text to list of integers\n",
        "raw_seq=[char_to_int[x] for x in raw_text]\n",
        "print(raw_seq[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "859zOvqpnwCN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(c) Divide into train and test. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giJUPXe1n3Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use 90% of data for training, rest for testing\n",
        "ntrain=int(0.9*len(raw_seq))\n",
        "train_seq=raw_seq[:ntrain]\n",
        "test_seq=raw_seq[ntrain:]\n",
        "print(len(train_seq),len(test_seq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jamOLwfEoWZD",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(d) Build training sequences of fixed length. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3j7gXhuoVT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert one long sequence into batches of seqlen\n",
        "def prepare_sequences(text,seqlen):\n",
        "  # number of sequences we can generate\n",
        "  nseq=(len(text)-1)//seqlen\n",
        "  # inputs = features\n",
        "  feats=np.zeros((nseq,seqlen))\n",
        "  # outputs = labels\n",
        "  labels=np.zeros((nseq,seqlen))\n",
        "  for i in range(nseq):\n",
        "    # inputs are chunks of the sequence\n",
        "    feats[i,:]=text[i*seqlen:i*seqlen+seqlen]\n",
        "    # labels are the same chunks but advanced by one unit \n",
        "    labels[i,:]=text[i*seqlen+1:i*seqlen+seqlen+1]\n",
        "  return feats,labels\n",
        "\n",
        "# prepare sequences of length 100\n",
        "seqlen=100\n",
        "Xtrain,ytrain = prepare_sequences(train_seq,seqlen)\n",
        "Xtest,ytest = prepare_sequences(test_seq,seqlen)\n",
        "\n",
        "print(Xtrain.shape,ytrain.shape)\n",
        "print(Xtest.shape,ytest.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N55UVqupzF6",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(e) Build the recurrent model. Run the code and add comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvQmEePLp0s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# this will be used by Keras to report perplexity during training\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = tf.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "    return perplexity\n",
        "\n",
        "# sizes of the problem and the embedding\n",
        "isize=NCHAR\n",
        "embed_size=64\n",
        "osize=NCHAR\n",
        "\n",
        "# build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=isize, output_dim=embed_size,input_length=seqlen))\n",
        "model.add(LSTM(128,return_sequences=True,activation='tanh'))\n",
        "model.add(LSTM(128,return_sequences=True,activation='tanh'))\n",
        "model.add(TimeDistributed(Dense(osize, activation='softmax')));\n",
        "#\n",
        "# compile the network\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=[perplexity])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8eDh2Xyq7gi",
        "colab_type": "text"
      },
      "source": [
        "(f) train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5QL8nhOq-2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "history=model.fit(Xtrain,ytrain, batch_size=64, validation_data=(Xtest,ytest), epochs=100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW8FHyjEAFiI",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(g) Calculate perplexity on test set. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj2m4HNmAODX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "# make predictions from the test data\n",
        "ypred=model.predict(Xtest)\n",
        "\n",
        "# reshape back into a single long sequence\n",
        "nseq=ypred.shape[0]\n",
        "seqlen=ypred.shape[1]\n",
        "ypred=np.reshape(ypred,(nseq*seqlen,ypred.shape[2]));\n",
        "\n",
        "# reshape the original test data back to a single sequence\n",
        "ytest_seq=np.reshape(ytest,(nseq*seqlen,1))\n",
        "\n",
        "# collect all the probabilities assigned to the correct words \n",
        "probs=[]\n",
        "for i in range(ytest_seq.shape[0]):\n",
        "  probs.append(ypred[i,int(ytest_seq[i])])\n",
        "\n",
        "# get the mean probability, entropy and perplexity\n",
        "meanprob=np.mean(probs)\n",
        "entropy=np.mean(-np.log(probs))\n",
        "print(meanprob,entropy,math.exp(entropy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YceLFtXlIPbx",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(h) Generate some new text using the language model. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNJroHB0ISXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample from a probability distribution\n",
        "def sampledist(dist):\n",
        "  thresh=np.random.random()\n",
        "  sum=0\n",
        "  for i in range(len(dist)):\n",
        "    sum += dist[i]\n",
        "    if sum > thresh:\n",
        "      return(i)\n",
        "  return(0)\n",
        "\n",
        "def generate_text(model, start_string, num_generate=1000):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char_to_int[s] for s in start_string]\n",
        "  input_eval = pad_sequences([input_eval],maxlen=100,padding='pre',value=0)\n",
        "  input_eval = np.reshape(input_eval,(1,100))\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # reset the state of the model\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    # generate the predictions of the model\n",
        "    predictions = model.predict(input_eval,batch_size=1)\n",
        "\n",
        "    # get the character probabilities for the last element\n",
        "    cprobabilities=predictions[0][-1,:]\n",
        "\n",
        "    # choose one of the characters by random sampling\n",
        "    predicted_id = sampledist(cprobabilities)\n",
        "  \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval[0,0:99] = input_eval[0,1:100];\n",
        "    input_eval[0,99] = predicted_id;\n",
        "\n",
        "    # convert the id to a character and save\n",
        "    text_generated.append(chars[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "# generate 1000 characters of Alice in Wonderland style text\n",
        "import textwrap\n",
        "text=generate_text(model, start_string=\"once upon a time \",num_generate=1000)\n",
        "print(textwrap.fill(text,80))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}