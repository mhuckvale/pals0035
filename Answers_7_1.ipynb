{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_7_1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaxAfBk1iWFo",
        "colab_type": "text"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 7.1 Answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKlBlwuGigPf",
        "colab_type": "text"
      },
      "source": [
        "(a) setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NyvFYGmiVgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, SimpleRNN, LSTM, GRU, Bidirectional, Dropout, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQzyn_HwkS62",
        "colab_type": "text"
      },
      "source": [
        "(b) read in text file and convert to integer list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq9BEcbFkXNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://www.phon.ucl.ac.uk/courses/pals0039/data/alice.txt\"\n",
        "response = requests.get(url)\n",
        "raw_text = response.text.lower().replace('\\n',' ')\n",
        "print(raw_text[:250])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWGzYeu_mYvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "NCHAR=len(chars)\n",
        "print(NCHAR)\n",
        "raw_seq=[char_to_int[x] for x in raw_text]\n",
        "print(raw_seq[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "859zOvqpnwCN",
        "colab_type": "text"
      },
      "source": [
        "(c) divide into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giJUPXe1n3Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntrain=int(0.9*len(raw_seq))\n",
        "train_seq=raw_seq[:ntrain]\n",
        "test_seq=raw_seq[ntrain:]\n",
        "print(len(train_seq),len(test_seq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jamOLwfEoWZD",
        "colab_type": "text"
      },
      "source": [
        "(d) build training sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3j7gXhuoVT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequences(text,seqlen):\n",
        "  nseq=(len(text)-1)//seqlen\n",
        "  feats=np.zeros((nseq,seqlen))\n",
        "  labels=np.zeros((nseq,seqlen))\n",
        "  for i in range(nseq):\n",
        "    feats[i,:]=text[i*seqlen:i*seqlen+seqlen]       # input is text sequence\n",
        "    labels[i,:]=text[i*seqlen+1:i*seqlen+seqlen+1]  # output is text sequence advanced by 1\n",
        "#  feats=np.zeros((nseq,seqlen,1))\n",
        "#  labels=np.zeros((nseq,seqlen,1))\n",
        "#  for i in range(nseq):\n",
        "#    feats[i,:,0]=text[i*seqlen:i*seqlen+seqlen]       # input is text sequence\n",
        "#    labels[i,:,0]=text[i*seqlen+1:i*seqlen+seqlen+1]  # output is text sequence advanced by 1\n",
        "  return feats,labels\n",
        "\n",
        "seqlen=100\n",
        "Xtrain,ytrain = prepare_sequences(train_seq,seqlen)\n",
        "Xtest,ytest = prepare_sequences(test_seq,seqlen)\n",
        "\n",
        "print(Xtrain.shape,ytrain.shape)\n",
        "print(Xtest.shape,ytest.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N55UVqupzF6",
        "colab_type": "text"
      },
      "source": [
        "(e) build a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvQmEePLp0s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = tf.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "    return perplexity\n",
        "\n",
        "osize=NCHAR\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=NCHAR, output_dim=64,input_length=seqlen))\n",
        "model.add(LSTM(256,return_sequences=True,activation='tanh'))\n",
        "model.add(LSTM(256,return_sequences=True,activation='tanh'))\n",
        "#model.add(LSTM(256,return_sequences=True,activation='tanh',input_shape=(seqlen,1)))\n",
        "model.add(TimeDistributed(Dense(osize, activation='softmax')));\n",
        "#\n",
        "# compile the network\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=[perplexity])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8eDh2Xyq7gi",
        "colab_type": "text"
      },
      "source": [
        "(f) train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5QL8nhOq-2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# train the model\n",
        "history=model.fit(Xtrain,ytrain, batch_size=64, validation_data=(Xtest,ytest), epochs=100)\n",
        "print(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW8FHyjEAFiI",
        "colab_type": "text"
      },
      "source": [
        "(g) calculate perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj2m4HNmAODX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "ypred=model.predict(Xtest)\n",
        "print(ypred.shape)\n",
        "nseq=ypred.shape[0]\n",
        "seqlen=ypred.shape[1]\n",
        "ypred=np.reshape(ypred,(nseq*seqlen,ypred.shape[2]));\n",
        "print(ypred.shape)\n",
        "\n",
        "ytest_seq=np.reshape(ytest,(nseq*seqlen,1))\n",
        "print(ytest_seq.shape)\n",
        "\n",
        "probs=[]\n",
        "for i in range(ytest_seq.shape[0]):\n",
        "  probs.append(ypred[i,int(ytest_seq[i])])\n",
        "meanprob=np.mean(probs)\n",
        "entropy=np.mean(-np.log(probs))\n",
        "print(meanprob,entropy,math.exp(entropy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YceLFtXlIPbx",
        "colab_type": "text"
      },
      "source": [
        "(h) generate some new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNJroHB0ISXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char_to_int[s] for s in start_string]\n",
        "  input_eval = pad_sequences([input_eval],maxlen=100,padding='pre',value=0)\n",
        "  input_eval = np.reshape(input_eval,(1,100))\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    #print(input_eval.shape)\n",
        "    #print(input_eval)\n",
        "    predictions = model.predict(input_eval,batch_size=1)\n",
        "    #print(predictions)\n",
        "\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    #print(predictions)\n",
        "\n",
        "    # using a categorical distribution to predict the word returned by the model\n",
        "#   predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(tf.math.log(predictions), num_samples=1)[-1,0].numpy()\n",
        "    #print(predicted_id)\n",
        "#    predicted_id = np.argmax(predictions)\n",
        "  \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval[0,0:99] = input_eval[0,1:100];\n",
        "    input_eval[0,99] = predicted_id;\n",
        "\n",
        "    text_generated.append(chars[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "import textwrap\n",
        "text=generate_text(model, start_string=\"once upon a time \")\n",
        "print(textwrap.fill(text,80))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}