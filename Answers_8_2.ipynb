{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_8_2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVRlTaZ250Cm",
        "colab_type": "text"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 8.2 Answers\n",
        "\n",
        "Exercise developed from: [Semantic similarity with TF-Hub Universal Encoder](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzeXN70754a1",
        "colab_type": "text"
      },
      "source": [
        "(a) set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrADKo2v5p_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6o6Z4QT6R4-",
        "colab_type": "text"
      },
      "source": [
        "(b) load sentence encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1TWf9OQ6UQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTqnNyRP6uFk",
        "colab_type": "text"
      },
      "source": [
        "(c) try out a few emebddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImNIkVNP6w5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "def print_embedding(messg):\n",
        "  print(\"Message:\",messg)\n",
        "  embedding = embed([messg])[0].numpy()\n",
        "  print(\"Embedding Size:\",len(embedding))\n",
        "  print(\"Embedding:\",embedding[:5],\"...\\n\")\n",
        "\n",
        "for messg in messages:\n",
        "  print_embedding(messg)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnWl3e4J8m60",
        "colab_type": "text"
      },
      "source": [
        "(d) Compare similarity of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4rmetcM8ptC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages = [\n",
        "    # Smartphones\n",
        "    \"I like my phone\",\n",
        "    \"My phone is not good.\",\n",
        "    \"Your cellphone looks great.\",\n",
        "\n",
        "    # Weather\n",
        "    \"Will it snow tomorrow?\",\n",
        "    \"Recently a lot of hurricanes have hit the US\",\n",
        "    \"Global warming is real\",\n",
        "\n",
        "    # Food and health\n",
        "    \"An apple a day, keeps the doctors away\",\n",
        "    \"Eating strawberries is healthy\",\n",
        "    \"Is paleo better than keto?\",\n",
        "\n",
        "    # Asking about age\n",
        "    \"How old are you?\",\n",
        "    \"what is your age?\",\n",
        "]\n",
        "\n",
        "message_embeddings = embed(messages)\n",
        "\n",
        "corr = np.inner(message_embeddings,message_embeddings)\n",
        "#print(corr)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(corr,origin='upper',cmap=\"YlOrRd\",aspect='auto')\n",
        "ax = plt.gca()\n",
        "ax.set_yticks(range(len(messages)))\n",
        "ax.set_yticklabels(messages)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cASov3K1_51Q",
        "colab_type": "text"
      },
      "source": [
        "(e) load a document classification task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY3XjQZK_3R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Trains and evaluate a simple MLP\n",
        "on the Reuters newswire topic classification task.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "max_words = 1000\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "dictionary=reuters.get_word_index();\n",
        "reverse_dictionary={0:\"padding\",1:\"BOS\",2:\"UNK\"}\n",
        "for (key,value) in dictionary.items():\n",
        "  reverse_dictionary[value+3]=key\n",
        "\n",
        "print(list(map(reverse_dictionary.get, range(10))))\n",
        "\n",
        "def reuters_text(seq):\n",
        "  text=[]\n",
        "  for widx in seq:\n",
        "    text.append(reverse_dictionary[widx])\n",
        "  return \" \".join(text)\n",
        "\n",
        "print(\"First review:\",reuters_text(x_train[0]))\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}