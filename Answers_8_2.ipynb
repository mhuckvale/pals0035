{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_8_2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVRlTaZ250Cm",
        "colab_type": "text"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 8.2 Answers\n",
        "\n",
        "In this exercise we use the Google Universal Sentence encoder to compute the similarity between sentences. The exercise was developed from: [Semantic similarity with TF-Hub Universal Encoder](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzeXN70754a1",
        "colab_type": "text"
      },
      "source": [
        "(a) Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrADKo2v5p_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6o6Z4QT6R4-",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(b) Load the sentence encoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1TWf9OQ6UQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the sentence encoder is available from the tensorflow hub service\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "\n",
        "# a function to calculate the embeddings\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTqnNyRP6uFk",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(c) Try out a few emebddings. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImNIkVNP6w5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example texts\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "# cakculate and print an embedding vector\n",
        "def print_embedding(messg):\n",
        "  print(\"Message:\",messg)\n",
        "  embedding = embed([messg])[0].numpy()\n",
        "  print(\"Embedding Size:\",len(embedding))\n",
        "  print(\"Embedding:\",embedding[:10],\"...\\n\")\n",
        "\n",
        "# print the embedding of each message \n",
        "for messg in messages:\n",
        "  print_embedding(messg)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnWl3e4J8m60",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(d) Compare similarity of sentences. Run the code then try out some sentences of your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4rmetcM8ptC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of sentences to embed and compare\n",
        "messages = [\n",
        "    # Smartphones\n",
        "    \"I like my phone\",\n",
        "    \"My phone is not good.\",\n",
        "    \"Your cellphone looks great.\",\n",
        "\n",
        "    # Weather\n",
        "    \"Will it snow tomorrow?\",\n",
        "    \"Recently a lot of hurricanes have hit the US\",\n",
        "    \"Global warming is real\",\n",
        "\n",
        "    # Food and health\n",
        "    \"An apple a day, keeps the doctors away\",\n",
        "    \"Eating strawberries is healthy\",\n",
        "    \"Is paleo better than keto?\",\n",
        "\n",
        "    # Asking about age\n",
        "    \"How old are you?\",\n",
        "    \"what is your age?\",\n",
        "]\n",
        "\n",
        "# calculate the embedded versions\n",
        "message_embeddings = embed(messages)\n",
        "\n",
        "# np.inner computes the dot product between all pairs of vectors.\n",
        "similarity = np.inner(message_embeddings,message_embeddings)\n",
        "#print(similarity)\n",
        "\n",
        "# plot the similarities as a figure\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(similarity,origin='upper',cmap=\"YlOrRd\",aspect='auto')\n",
        "ax = plt.gca()\n",
        "ax.set_yticks(range(len(messages)))\n",
        "ax.set_yticklabels(messages)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}