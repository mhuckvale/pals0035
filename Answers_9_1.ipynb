{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_9_1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_9_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkhqPoPYpe9i",
        "colab_type": "text"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 9.1 Answers\n",
        "\n",
        "In this exercise we develop a simple chatbot application using a sequence-to-sequence model and a database of movie dialogues. The exercise was developed from [https://github.com/sekharvth/simple-chatbot-keras](https://github.com/sekharvth/simple-chatbot-keras)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkCmB_qrq3PO",
        "colab_type": "text"
      },
      "source": [
        "(a) Import the library modules we will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5HFo_TYpefW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed\n",
        "from tensorflow.keras.models import Model, load_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRRLn5DMrALF",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(b) Import movie dialogues data set. This comes from [https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZB4oDIfrDsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the original corpus has been converted to CSV format\n",
        "df=pd.read_csv(\"https://www.phon.ucl.ac.uk/courses/pals0039/data/movie_lines.csv\",keep_default_na=False)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-63m4HaBxK5x",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(c) Tokenize the dialogues. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5rRvfhAxOn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the vocabulary size\n",
        "max_words=5000\n",
        "\n",
        "# convert all the contexts to a list to analyze vocabulary\n",
        "contexts=df.CONTEXT.tolist()\n",
        "# convert all the targets to a list to analyze vocabulary\n",
        "targets=[ \"BOS \"+l+\" EOS\" for l in df.TARGET.tolist()]\n",
        "print(\"Contexts:\",contexts[:5])\n",
        "print(\"Targets:\",targets[:5])\n",
        "\n",
        "# use the Keras tokenizer to build a vocaulary\n",
        "tokenizer = Tokenizer(num_words=max_words,oov_token=\"UNK\")\n",
        "tokenizer.fit_on_texts(df.CONTEXT.tolist()+targets)\n",
        "word_index=tokenizer.word_index\n",
        "print(\"Found\",len(word_index),\"different words.\")\n",
        "\n",
        "print(list(word_index.items())[:10])\n",
        "print(list(word_index.items())[-10:])\n",
        "\n",
        "# now use the tokenizer to convert all word sequences to indices\n",
        "ctxt=tokenizer.texts_to_sequences(df.CONTEXT.tolist())\n",
        "targ=tokenizer.texts_to_sequences(targets)\n",
        "print(\"Context\",ctxt[:5])\n",
        "print(\"Target\",targ[:5])\n",
        "\n",
        "# build a reverse index from indices back to words\n",
        "index_to_word={ v:k for k,v in tokenizer.word_index.items()}\n",
        "index_to_word[0]='.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvrN63Hc0feD",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(d) Reduce the complete data set to only simple dialogue turns. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tf_-iwc053h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set limits to length of turns\n",
        "min_seq=2\n",
        "max_seq=12\n",
        "\n",
        "# filter out all dialogues containing fewer than 2 words or more than 12 words or contain unknown words\n",
        "print(\"Unfiltered count\",len(ctxt),len(targ))\n",
        "ctxt_filt=[]\n",
        "targ_filt=[];\n",
        "for i in range(len(ctxt)):\n",
        "  clen=len(ctxt[i])\n",
        "  tlen=len(targ[i])-2   # -2 for BOS/EOS\n",
        "  if ((min_seq<=clen)and(clen<=max_seq)and(min_seq<=tlen)and(tlen<=max_seq)):\n",
        "    if (not (1 in ctxt[i]) and not (1 in targ[i])):       # 1 is code for UNK\n",
        "      ctxt_filt.append(ctxt[i])\n",
        "      targ_filt.append(targ[i])\n",
        "print(\"Filtered count\",len(ctxt_filt),len(targ_filt))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ7pMYun0l3C",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(e) Prepare data for training. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah2QDIRd2fIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create padded versions and decoder inputs for training\n",
        "seq_len=max_seq\n",
        "ctxt_pad=pad_sequences(ctxt_filt, maxlen=seq_len, padding='pre')\n",
        "targ_pad=pad_sequences(targ_filt, maxlen=seq_len+2, padding='post')\n",
        "outs_pad=np.roll(targ_pad,-1,axis=1)\n",
        "outs_pad[:,-1]=0\n",
        "\n",
        "# reduce number of training samples to speed up learning in demo\n",
        "ntrain=12800\n",
        "perm=np.random.permutation(len(ctxt_filt))\n",
        "ctxt_pad=ctxt_pad[perm[:ntrain]]\n",
        "targ_pad=targ_pad[perm[:ntrain]]\n",
        "outs_pad=outs_pad[perm[:ntrain]]\n",
        "\n",
        "# print a few examples of what we have\n",
        "print(\"Context\",ctxt_pad[:5])\n",
        "print(\"Target\",targ_pad[:5])\n",
        "print(\"Outputs\",outs_pad[:5])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB35OMpp_Toj",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(f) Load Glove embeddings to use as input to network. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prLLl1yD_WAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the Glove embeddings\n",
        "df=pd.read_csv('https://www.phon.ucl.ac.uk/courses/pals0039/data/glove.6B.100d.zip',header=None)\n",
        "df.rename(columns={0:\"word\"},inplace=True)\n",
        "print(\"Read %d word embeddings of length %d\" % (len(df),len(df.columns)-1))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhMIeNlm_Y7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build an index into the embeddings\n",
        "glove_index={}\n",
        "for i,word in enumerate(df.word):\n",
        "  glove_index[word]=i\n",
        "\n",
        "# build an embedding matrix for words in movie dialogues\n",
        "embed_dim=100\n",
        "word_embed=np.zeros((max_words,embed_dim))\n",
        "oov_count=0\n",
        "for i in range(max_words):\n",
        "  w=index_to_word[i]\n",
        "  if w in glove_index:\n",
        "    # found movie word in glove\n",
        "    idx=glove_index[w]\n",
        "  else:\n",
        "    # failed to find movie word in glove\n",
        "    idx=glove_index[\".\"]\n",
        "    oov_count+=1\n",
        "  word_embed[i,:]=np.array(df.iloc[idx,1:])\n",
        "\n",
        "# report out-of-vocabulary rate in Glove\n",
        "print(\"OOV rate = %.1f%%\" % (100*oov_count/max_words))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2KSiAGFUu9",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(g) Build the sequence to sequence model. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjJkxZA0FvyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up basic parameters\n",
        "latent_dim=200\n",
        "num_encoder_tokens=max_words\n",
        "num_decoder_tokens=max_words\n",
        "\n",
        "# input_context is the encoder input\n",
        "input_context = Input(shape = (seq_len, ), dtype = 'int32', name = 'input_context')\n",
        "# input target is the decoder input\n",
        "input_target = Input(shape = (seq_len+2, ), dtype = 'int32', name = 'input_target')\n",
        "\n",
        "# create the input embedding and set it to the weights found by Glove\n",
        "embed_layer = Embedding(input_dim = max_words, output_dim = embed_dim, trainable = False )\n",
        "embed_layer.build((None,))\n",
        "embed_layer.set_weights([word_embed])\n",
        "\n",
        "# we use the same embedding layer for both encoder and decoder inputs\n",
        "input_ctx_embed = embed_layer(input_context)\n",
        "embed_layer2 = Embedding(input_dim = max_words, output_dim = embed_dim, trainable = True )\n",
        "input_tar_embed = embed_layer2(input_target)\n",
        "\n",
        "# encoder LSTM takes input embedding and just returns final state\n",
        "LSTM_encoder = LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = LSTM_encoder(input_ctx_embed)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "LSTM_decoder = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = LSTM_decoder(input_tar_embed,initial_state=encoder_states)\n",
        "decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "train_model = Model([input_context, input_target], decoder_outputs)\n",
        "\n",
        "train_model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "train_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWQvbqAfGrGK",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(h) Train the model and build the encoder and decoder models we'll need for inference. Run the code and add comments. Training will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUeVVtivGsc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# train the model\n",
        "train_model.fit([ctxt_pad, targ_pad], outs_pad, epochs = 100, batch_size = 128)\n",
        "\n",
        "# build the encoder model\n",
        "encoder_model = Model(input_context, encoder_states)\n",
        "\n",
        "# build the decoder model\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = LSTM_decoder(input_tar_embed, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([input_target,decoder_state_input_h,decoder_state_input_c],[decoder_outputs,state_h,state_c])\n",
        "\n",
        "# save the models, since training can take some time\n",
        "encoder_model.save('ex9_1_encoder.h5')\n",
        "decoder_model.save('ex9_1_decoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_jQc6riVdJX",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(i) Define inference operation for dialogue turns. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHuIGSkvVhZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reload the models from the save files\n",
        "encoder_model=load_model('ex9_1_encoder.h5', compile=False)\n",
        "decoder_model=load_model('ex9_1_decoder.h5', compile=False)\n",
        "\n",
        "# function to generate a target from a context, i.e. to reply to a dialogue turn\n",
        "def decode_sequence(input_seq):\n",
        "  # Encode the input into a state vector\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, seq_len+2))\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  target_seq[0, 0] = tokenizer.word_index['bos']\n",
        "\n",
        "  # Sampling loop for a batch of sequences\n",
        "  decoded_sentence = []\n",
        "  pos=0\n",
        "  while True:\n",
        "    # get the outputs for all contiunations of the output prefix\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # get the most likely word at the current position\n",
        "    sampled_token_index = np.argmax(output_tokens[0, pos, :])\n",
        "\n",
        "    # exit condition: either hit max length or find stop character.\n",
        "    if (sampled_token_index == tokenizer.word_index['eos'] or pos >= seq_len):\n",
        "      break\n",
        "\n",
        "    # save word\n",
        "    decoded_sentence.append(index_to_word[1+sampled_token_index])\n",
        "\n",
        "    # Update the target sequence\n",
        "    pos += 1\n",
        "    target_seq[0, pos] = sampled_token_index\n",
        "\n",
        "  return \" \".join(decoded_sentence)\n",
        "\n",
        "# test it out on a few examples\n",
        "for i in range(10):\n",
        "  inp=[index_to_word[w] for w in ctxt_filt[i]]\n",
        "  print(\"Prompt\",i,' '.join(inp))\n",
        "  tar=[index_to_word[w] for w in targ_filt[i]]\n",
        "  print(\"Target\",i,' '.join(tar))\n",
        "  ctxt=ctxt_pad[i:i+1,:]\n",
        "  print(\"Output\",i,decode_sequence(ctxt),'\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILOxiDMUBi64",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(j) Interactive chat with the chatbot. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUbpCWOPBnZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a question\n",
        "print(\"Type 'stop' to stop.\")\n",
        "question=input(\"Q: \")\n",
        "while question != \"stop\":\n",
        "  # convert input sentence to list of word indices\n",
        "  ques_list=tokenizer.texts_to_sequences([question])\n",
        "  # pad the sequence\n",
        "  ques_pad=pad_sequences(ques_list, maxlen=seq_len, padding='pre')\n",
        "  # find response and print it\n",
        "  print(\"A:\",decode_sequence(ques_pad))\n",
        "  question=input(\"Q: \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK-vtVfykLhU",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "(k) This example could be improved in many ways: increase the number of training samples, change the size of the encoder/decoder or add additional layers, change the amount of training, perform filtering to eliminate words words unknown in the Glove data. There are also better ways to calculate performance - ignoring padding would be a start - or you could add separate test data. Do try out some variants yourself."
      ]
    }
  ]
}