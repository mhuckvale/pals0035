{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_9_1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_9_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkhqPoPYpe9i",
        "colab_type": "text"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 9.1 Answers\n",
        "\n",
        "Exercise developed from [https://github.com/sekharvth/simple-chatbot-keras](https://github.com/sekharvth/simple-chatbot-keras)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkCmB_qrq3PO",
        "colab_type": "text"
      },
      "source": [
        "(a) Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5HFo_TYpefW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed\n",
        "from tensorflow.keras.models import Model, load_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRRLn5DMrALF",
        "colab_type": "text"
      },
      "source": [
        "(b) import movie dialogues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZB4oDIfrDsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(\"https://www.phon.ucl.ac.uk/courses/pals0039/data/movie_lines.csv\",keep_default_na=False)\n",
        "print(df.describe())\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-63m4HaBxK5x",
        "colab_type": "text"
      },
      "source": [
        "(c) Tokenize the dialogues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5rRvfhAxOn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words=5000\n",
        "\n",
        "contexts=df.CONTEXT.tolist()\n",
        "targets=[ \"BOS \"+l+\" EOS\" for l in df.TARGET.tolist()]\n",
        "print(\"Contexts:\",contexts[:5])\n",
        "print(\"Targets:\",targets[:5])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words,oov_token=\"UNK\")\n",
        "tokenizer.fit_on_texts(df.CONTEXT.tolist()+targets)\n",
        "word_index=tokenizer.word_index\n",
        "print(\"Found\",len(word_index),\"different words.\")\n",
        "\n",
        "print(list(word_index.items())[:10])\n",
        "print(list(word_index.items())[-10:])\n",
        "\n",
        "ctxt=tokenizer.texts_to_sequences(df.CONTEXT.tolist())\n",
        "targ=tokenizer.texts_to_sequences(targets)\n",
        "print(\"Context\",ctxt[:5])\n",
        "print(\"Target\",targ[:5])\n",
        "\n",
        "# build a reverse index\n",
        "index_to_word={ v:k for k,v in tokenizer.word_index.items()}\n",
        "index_to_word[0]='.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvrN63Hc0feD",
        "colab_type": "text"
      },
      "source": [
        "(d) filter out all dialogues containing fewer than 2 words or more than 12 words or contain unknown words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tf_-iwc053h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_seq=2\n",
        "max_seq=12\n",
        "\n",
        "print(\"Unfiltered count\",len(ctxt),len(targ))\n",
        "ctxt_filt=[]\n",
        "targ_filt=[];\n",
        "for i in range(len(ctxt)):\n",
        "  clen=len(ctxt[i])\n",
        "  tlen=len(targ[i])-2   # -2 for BOS/EOS\n",
        "  if ((min_seq<=clen)and(clen<=max_seq)and(min_seq<=tlen)and(tlen<=max_seq)):\n",
        "    if (not (1 in ctxt[i]) and not (1 in targ[i])):       # 1 is code for UNK\n",
        "      ctxt_filt.append(ctxt[i])\n",
        "      targ_filt.append(targ[i])\n",
        "print(\"Filtered count\",len(ctxt_filt),len(targ_filt))\n",
        "\n",
        "#ucount=0;\n",
        "#for i in range(len(ctxt_filt)):\n",
        "#  if ((1 in ctxt_filt[i])or(1 in targ_filt[i])):\n",
        "#    ucount+=1\n",
        "#print(\"Dialogues with UNK:\",ucount)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ7pMYun0l3C",
        "colab_type": "text"
      },
      "source": [
        "(e) prepare data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah2QDIRd2fIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len=max_seq\n",
        "ctxt_pad=pad_sequences(ctxt_filt, maxlen=seq_len, padding='post')\n",
        "targ_pad=pad_sequences(targ_filt, maxlen=seq_len+2, padding='post')\n",
        "outs_pad=np.roll(targ_pad,-1,axis=1)\n",
        "outs_pad[:,-1]=0\n",
        "print(\"Context\",ctxt_pad[:5])\n",
        "print(\"Target\",targ_pad[:5])\n",
        "print(\"Outputs\",outs_pad[:5])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB35OMpp_Toj",
        "colab_type": "text"
      },
      "source": [
        "(f) load Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prLLl1yD_WAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('https://www.phon.ucl.ac.uk/courses/pals0039/data/glove.6B.100d.zip',header=None)\n",
        "df.rename(columns={0:\"word\"},inplace=True)\n",
        "print(\"Read %d word embeddings of length %d\" % (len(df),len(df.columns)-1))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhMIeNlm_Y7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build an index into the embeddings\n",
        "glove_index={}\n",
        "for i,word in enumerate(df.word):\n",
        "  glove_index[word]=i\n",
        "\n",
        "# build an embedding matrix for words in movie dialogues\n",
        "embed_dim=100\n",
        "word_embed=np.zeros((max_words,embed_dim))\n",
        "oov_count=0\n",
        "for i in range(max_words):\n",
        "  w=index_to_word[i]\n",
        "  if w in glove_index:\n",
        "    idx=glove_index[w]\n",
        "  else:\n",
        "    idx=glove_index[\".\"]\n",
        "    oov_count+=1\n",
        "  word_embed[i,:]=np.array(df.iloc[idx,1:])\n",
        "\n",
        "print(\"OOV rate = %.1f%%\" % (100*oov_count/max_words))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2KSiAGFUu9",
        "colab_type": "text"
      },
      "source": [
        "(g) Build model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjJkxZA0FvyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "latent_dim=300\n",
        "num_encoder_tokens=max_words\n",
        "num_decoder_tokens=max_words\n",
        "\n",
        "# model inputs\n",
        "input_context = Input(shape = (seq_len, ), dtype = 'int32', name = 'input_context')\n",
        "input_target = Input(shape = (seq_len+2, ), dtype = 'int32', name = 'input_target')\n",
        "\n",
        "# initial embedding\n",
        "embed_layer = Embedding(input_dim = max_words, output_dim = embed_dim, trainable = False )\n",
        "embed_layer.build((None,))\n",
        "embed_layer.set_weights([word_embed])\n",
        "\n",
        "# same embedding layer used for both inputs\n",
        "input_ctx_embed = embed_layer(input_context)\n",
        "input_tar_embed = embed_layer(input_target)\n",
        "\n",
        "# encoder LSTM takes input embedding and just returns final state\n",
        "LSTM_encoder = LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = LSTM_encoder(input_ctx_embed)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "LSTM_decoder = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = LSTM_decoder(input_tar_embed,initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "train_model = Model([input_context, input_target], decoder_outputs)\n",
        "\n",
        "train_model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "train_model.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWQvbqAfGrGK",
        "colab_type": "text"
      },
      "source": [
        "(h) train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUeVVtivGsc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model.fit([ctxt_pad, targ_pad], outs_pad, epochs = 40, batch_size = 128)\n",
        "\n",
        "encoder_model = Model(input_context, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = LSTM_decoder(input_tar_embed, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([input_target] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
        "\n",
        "encoder_model.save('ex9_1_encoder.h5')\n",
        "decoder_model.save('ex9_1_decoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_jQc6riVdJX",
        "colab_type": "text"
      },
      "source": [
        "(i) build decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHuIGSkvVhZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model=load_model('ex9_1_encoder.h5')\n",
        "decoder_model=load_model('ex9_1_decoder.h5')\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # Encode the input as state vectors.\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, seq_len+2))\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  target_seq[0, 0] = tokenizer.word_index['bos']\n",
        "\n",
        "  # Sampling loop for a batch of sequences\n",
        "  decoded_sentence = []\n",
        "  while True:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # Sample a token\n",
        "    sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "    # Exit condition: either hit max length or find stop character.\n",
        "    if (sampled_token_index == tokenizer.word_index['eos'] or len(decoded_sentence) > seq_len):\n",
        "      break\n",
        "\n",
        "    # save word\n",
        "    decoded_sentence.append(index_to_word[sampled_token_index])\n",
        "\n",
        "    # Update the target sequence (of length 1).\n",
        "    target_seq = np.zeros((1, seq_len+2))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # Update states\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return \" \".join(decoded_sentence)\n",
        "\n",
        "# get a question\n",
        "#question=input(\"Ask something: \")\n",
        "#ques_list=tokenizer.texts_to_sequences([question])\n",
        "#ques_pad=pad_sequences(ques_list, maxlen=seq_len, padding='post')\n",
        "#print(ques_pad)\n",
        "\n",
        "# print the answer generated for the given question\n",
        "#print(\"Q:\",question)\n",
        "#print(\"A:\",decode_sequence(ques_pad))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILOxiDMUBi64",
        "colab_type": "text"
      },
      "source": [
        "(j) chat with chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUbpCWOPBnZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a question\n",
        "print(\"Type 'stop' to stop.\")\n",
        "question=input(\"Q: \")\n",
        "while question != \"stop\":\n",
        "  ques_list=tokenizer.texts_to_sequences([question])\n",
        "  ques_pad=pad_sequences(ques_list, maxlen=seq_len, padding='post')\n",
        "  print(\"A:\",decode_sequence(ques_pad))\n",
        "  question=input(\"Q: \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}