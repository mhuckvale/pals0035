{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_4_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_4_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba_u4lIeY3lI"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "#Exercise 4.2 Answers\n",
        "\n",
        "In this exercise we implement a DNN for recognition of emotion from speech.\n",
        "\n",
        "The data comes from recordings made at the [Enterface05 summer school](http://www.enterface.net/).\n",
        "\n",
        "The audio recordings have been extracted from the video files, and each has been processed through the [OpenSMILE](https://www.audeering.com/opensmile/) feature extractor. Each recording is represented by a fixed-length vector of 6373 features. Every recording also has an emotion label from the set <tt>['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']</tt> which was the target emotion given to each speaker. The code loads the data from a CSV file, encodes the emotion classes as numbers, the selects random subsets for training and testing. We then build a DNN classifier and display its performance as a confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr7p4QeSY3zH"
      },
      "source": [
        "---\r\n",
        "(a) Set-up. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3zsmTrycoqU"
      },
      "source": [
        "# import standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import Keras library\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEvOtgoFcvtL"
      },
      "source": [
        "---\n",
        "(b) Download the data set and measure its parameters. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMMVpaDcculb"
      },
      "source": [
        "# down load the emotion corpus spreadsheet\n",
        "df=pd.read_csv(\"https://www.phon.ucl.ac.uk/courses/pals0039/data/emotion.csv\",sep=',')\n",
        "\n",
        "# print size\n",
        "print(\"Number of rows=\",len(df))\n",
        "print(\"Number of columns=\",len(df.columns))\n",
        "\n",
        "# print first rows\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql_VI2a5du-E"
      },
      "source": [
        "---\n",
        "(c) Convert data and labels into numpy arrays. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0VXwrrDd0yN"
      },
      "source": [
        "# convert the EMOTION column to numbers\n",
        "df['EMOTION']=pd.Categorical(df['EMOTION'])\n",
        "\n",
        "# get a list of the named categories\n",
        "emolist=list(df['EMOTION'].cat.categories)\n",
        "print(emolist)\n",
        "\n",
        "# convert data frame to numpy arrays - selecting training features\n",
        "Xdata=np.array(df.iloc[:,4:])\n",
        "ydata=np.array(df['EMOTION'].cat.codes)\n",
        "\n",
        "# get total number of rows\n",
        "ndata=Xdata.shape[0]\n",
        "# get random order\n",
        "p=np.random.permutation(ndata)\n",
        "# train on 90%\n",
        "ntrain=int(0.9*ndata)\n",
        "\n",
        "# divide into training and test data\n",
        "Xtrain=Xdata[p[:ntrain],:]\n",
        "Xtest=Xdata[p[ntrain:],:]\n",
        "ytrain=ydata[p[:ntrain]]\n",
        "ytest=ydata[p[ntrain:]]\n",
        "\n",
        "print(Xtrain[:10,:20])\n",
        "print(ytrain[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yapUMnnyhqYv"
      },
      "source": [
        "---\n",
        "(d) Build the DNN model. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dar4sx0Thvap"
      },
      "source": [
        "# get some basic sizes of inputs and outputs\n",
        "isize=Xtrain.shape[1];\n",
        "osize=len(emolist)\n",
        "print(\"inputs\",isize,\"outputs\",osize)\n",
        "\n",
        "# use Keras sequentioal model\n",
        "model = Sequential()\n",
        "# add dense layer with isize inputs\n",
        "model.add(Dense(64,activation='tanh',input_shape=(isize,)))\n",
        "# add a hidden layer\n",
        "model.add(Dense(16,activation='tanh'));\n",
        "# add the output layer as set of class probabilities\n",
        "model.add(Dense(osize, activation='softmax'))\n",
        "# compile model using the sparse cross-entropy, which allows us to present the classes as integers\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F17DebXPieNj"
      },
      "source": [
        "---\n",
        "(e) Train the model. Run the code and add comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8IIHPEsiiUi"
      },
      "source": [
        "# train the model, using 10% as validation data\n",
        "history=model.fit(Xtrain,ytrain, epochs=10, batch_size=64, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0SPdgeji-E7"
      },
      "source": [
        "---\n",
        "(f) Evaluate the model on the test data. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGgikOsBi9Cb"
      },
      "source": [
        "# evaluate the model\n",
        "loss,accuracy=model.evaluate(Xtest,ytest)\n",
        "print(\"Loss\",loss,\"Accuracy\",accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRVl6cJTjOhi"
      },
      "source": [
        "---\n",
        "(g) Print confusion matrix. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxAFzNMqjWUT"
      },
      "source": [
        "# get predictions of the model\n",
        "ypred=model.predict(Xtest)\n",
        "\n",
        "# use argmax to choose most probable\n",
        "ypred=np.argmax(ypred,axis=1)\n",
        "\n",
        "# set up confusion matrix\n",
        "counts=np.zeros((osize,osize))\n",
        "correct=0\n",
        "# compare predictions with correct answer\n",
        "for i in range(len(ytest)):\n",
        "  if (ypred[i]==ytest[i]):\n",
        "    correct += 1\n",
        "  counts[ytest[i],ypred[i]] += 1\n",
        "\n",
        "# print the confusions\n",
        "print(emolist)\n",
        "print(counts)\n",
        "print(\"Correct %.1f%%\" % (100*correct/len(ytest)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dip-6Zkc9Zu"
      },
      "source": [
        "---\r\n",
        "(h) Experiment with the example to try and improve performance. Here are some ideas:\r\n",
        "<ol>\r\n",
        "<li>Change the size and structure of the network.\r\n",
        "<li>Change the training regime.\r\n",
        "<li>(advanced) Select a subset of the 6373 features according to how much they vary between emotion classes in the training set. Eliminate the unused features (those that don't vary much across classes) from both training and testing data.\r\n",
        "</ol>"
      ]
    }
  ]
}