{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_7_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Exercise_7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaxAfBk1iWFo"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 7.1\n",
        "\n",
        "In this exercise we train a character sequence language model using a recurrent neural network and then generate some random text.\n",
        "\n",
        "We will use the text of Alice in Wonderland as the source material. Our language model will treat each character as a symbol. We divide the text into 95% for training and 5% for testing. Then split each part into sequences of length 100 characters for training and testing the model.\n",
        "\n",
        "After training we will calculate the perplexity of the model on the test data.\n",
        "Then we will use the model to generate some \"Alice-like\" text at the character level.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKlBlwuGigPf"
      },
      "source": [
        "(a) Import the standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NyvFYGmiVgd"
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, SimpleRNN, LSTM, GRU, Bidirectional, Dropout, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQzyn_HwkS62"
      },
      "source": [
        "---\n",
        "(b) Read in the Alice text and convert characters to a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq9BEcbFkXNW"
      },
      "source": [
        "# download the text file of alice in wonderland\n",
        "url = \"https://www.phon.ucl.ac.uk/courses/pals0039/data/alice.txt\"\n",
        "response = requests.get(url)\n",
        "# convert everything to lower case and replace newlines with spaces.\n",
        "raw_text = response.text.lower().replace('\\n',' ')\n",
        "print(raw_text[:250])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWGzYeu_mYvm"
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "# get a set of all characters\n",
        "chars = sorted(list(set(raw_text)))\n",
        "# build a dictionary that maps characters to integers\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "# get number of different characters\n",
        "NCHAR=len(chars)\n",
        "print(\"number of symbols\",NCHAR)\n",
        "# convert Alice text to list of integers\n",
        "raw_seq=[char_to_int[x] for x in raw_text]\n",
        "print(raw_seq[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "859zOvqpnwCN"
      },
      "source": [
        "---\n",
        "(c) Divide the enumerated text into train and test partitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giJUPXe1n3Cj"
      },
      "source": [
        "# use 95% of data for training, rest for testing\n",
        "ntrain=int(0.95*len(raw_seq))\n",
        "train_seq=raw_seq[:ntrain]\n",
        "test_seq=raw_seq[ntrain:]\n",
        "print(\"Train length\",len(train_seq),\"Test length\",len(test_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jamOLwfEoWZD"
      },
      "source": [
        "---\n",
        "(d) Split the training text up into training sequences of fixed length. Here we also calculate the output labels for each sequence, which are just the input characters shifted back one place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3j7gXhuoVT9"
      },
      "source": [
        "# function to convert one long sequence into batches of seqlen\n",
        "def prepare_sequences(text,seqlen):\n",
        "  # number of sequences we can generate\n",
        "  nseq=(len(text)-1)//seqlen\n",
        "  # inputs = features\n",
        "  feats=np.zeros((nseq,seqlen))\n",
        "  # outputs = labels\n",
        "  labels=np.zeros((nseq,seqlen))\n",
        "  for i in range(nseq):\n",
        "    # inputs are chunks of the sequence\n",
        "    feats[i,:]=text[i*seqlen:i*seqlen+seqlen]\n",
        "    # labels are the same chunks but advanced by one unit \n",
        "    labels[i,:]=text[i*seqlen+1:i*seqlen+seqlen+1]\n",
        "  return feats,labels\n",
        "\n",
        "# prepare sequences of length 100\n",
        "seqlen=100\n",
        "Xtrain,ytrain = prepare_sequences(train_seq,seqlen)\n",
        "Xtest,ytest = prepare_sequences(test_seq,seqlen)\n",
        "\n",
        "print(\"Train\",Xtrain.shape,ytrain.shape)\n",
        "print(\"Test\",Xtest.shape,ytest.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N55UVqupzF6"
      },
      "source": [
        "---\n",
        "(e) Build the recurrent model and add perplexity metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvQmEePLp0s_"
      },
      "source": [
        "import tensorflow as tf\n",
        "# define a function for Keras to calculate perplexity during training\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = tf.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "    return perplexity\n",
        "\n",
        "# sizes of the problem and the embedding to use for each character\n",
        "isize=NCHAR\n",
        "embed_size=32\n",
        "osize=NCHAR\n",
        "\n",
        "# build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=isize, output_dim=embed_size,input_length=seqlen))\n",
        "model.add(LSTM(128,return_sequences=True,activation='tanh'))\n",
        "model.add(LSTM(128,return_sequences=True,activation='tanh'))\n",
        "model.add(TimeDistributed(Dense(osize, activation='softmax')));\n",
        "#\n",
        "# compile the network\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=[perplexity])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8eDh2Xyq7gi"
      },
      "source": [
        "(f) Train the character language model and plot training progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5QL8nhOq-2o"
      },
      "source": [
        "# train the model\n",
        "history=model.fit(Xtrain,ytrain, batch_size=64, validation_split=0.05, epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlWg9PXNQ8_F"
      },
      "source": [
        "# get the history dictionary \n",
        "hist=history.history\n",
        "epochs=range(1,len(hist['loss'])+1)\n",
        "\n",
        "# plot loss curves\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs,hist['loss'],'ro',label=\"Training loss\")\n",
        "plt.plot(epochs,hist['val_loss'],'b-',label=\"Validation loss\")\n",
        "plt.ylim(bottom=0);\n",
        "plt.title(\"Training and Validation loss\",fontsize=16)\n",
        "plt.xlabel(\"Epochs\",fontsize=14)\n",
        "plt.ylabel(\"Loss\",fontsize=14)\n",
        "plt.grid()\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "# plot perplexity curves\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs,hist['perplexity'],'ro',label=\"Training perplexity\")\n",
        "plt.plot(epochs,hist['val_perplexity'],'b-',label=\"Validation perplexity\")\n",
        "plt.ylim(bottom=0);\n",
        "plt.title(\"Training and Validation perplexity\",fontsize=16)\n",
        "plt.xlabel(\"Epochs\",fontsize=14)\n",
        "plt.ylabel(\"Perplexity\",fontsize=14)\n",
        "plt.grid()\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW8FHyjEAFiI"
      },
      "source": [
        "---\n",
        "(g) Calculate the perplexity of the test set given the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj2m4HNmAODX"
      },
      "source": [
        "import math\n",
        "\n",
        "# make predictions from the test data\n",
        "ypred=model.predict(Xtest)\n",
        "\n",
        "# reshape back into a single long sequence\n",
        "nseq=ypred.shape[0]\n",
        "seqlen=ypred.shape[1]\n",
        "ypred=np.reshape(ypred,(nseq*seqlen,ypred.shape[2]));\n",
        "\n",
        "# reshape the original test data back to a single sequence\n",
        "ytest_seq=np.reshape(ytest,(nseq*seqlen,1))\n",
        "\n",
        "# collect all the probabilities assigned to the actual words \n",
        "probs=[]\n",
        "for i in range(ytest_seq.shape[0]):\n",
        "  probs.append(ypred[i,int(ytest_seq[i])])\n",
        "\n",
        "# get the mean probability, entropy and perplexity\n",
        "meanprob=np.mean(probs)\n",
        "entropy=np.mean(-np.log(probs))\n",
        "print(\"mean probability\",meanprob,\"entropy\",entropy,\"perplexity\",math.exp(entropy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YceLFtXlIPbx"
      },
      "source": [
        "---\n",
        "(h) Generate some new text character by character using the language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNJroHB0ISXo"
      },
      "source": [
        "# sample from a probability distribution over characters\n",
        "def sampledist(dist):\n",
        "  dist=dist/np.sum(dist)      # ensure normalised\n",
        "  thresh=np.random.random()   # choose a random variate\n",
        "  sum=0\n",
        "  for i in range(len(dist)):\n",
        "    sum += dist[i]\n",
        "    if sum > thresh:          # choose one value from the distribution\n",
        "      return(i)\n",
        "  return(0)\n",
        "\n",
        "# function to generate text from the model\n",
        "def generate_text(model, start_string, num_generate=1000):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Convert our start string to numbers (vectorizing)\n",
        "  # we put the start string at the end of the sequence\n",
        "  input_eval = [char_to_int[s] for s in start_string]\n",
        "  input_eval = pad_sequences([input_eval],maxlen=100,padding='pre',value=0)\n",
        "  input_eval = np.reshape(input_eval,(1,100))\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # reset the state of the model\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    # generate the predictions of the model\n",
        "    predictions = model.predict(input_eval,batch_size=1)\n",
        "\n",
        "    # get the character probabilities for the last element\n",
        "    cprobabilities=predictions[0][-1,:]\n",
        "\n",
        "    # choose one of the characters by random sampling\n",
        "    predicted_id = sampledist(cprobabilities)\n",
        "  \n",
        "    # We pass the predicted symbol as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval[0,0:99] = input_eval[0,1:100];\n",
        "    input_eval[0,99] = predicted_id;\n",
        "\n",
        "    # convert the id to a character and save\n",
        "    text_generated.append(chars[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "# generate 1000 characters of Alice in Wonderland style text\n",
        "import textwrap\n",
        "text=generate_text(model, start_string=\"once upon a time \",num_generate=1000)\n",
        "print(textwrap.fill(text,80))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_BpyvvgEy8w"
      },
      "source": [
        "(i) Experiment with the recurrent network structure and training to see if you can reduce the perplexity of the test set given the model to a value below 4. Does a better model lead to better generated text?"
      ]
    }
  ]
}