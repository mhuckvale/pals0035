{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answers_4_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Answers_4_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is1p5hg4EqlY"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 4.1 Answers\n",
        "\n",
        "In this exercise we implement a DNN system for sentiment analysis of movie reviews.\n",
        "\n",
        "We use a set of film reviews taken from the [Internet Movie Database](https://www.imdb.com/) which have been labelled as positive or negative. Words in the reviews have already been tokenised and encoded as numbers using a dictionary. We load the numeric sequences as variable length lists then build a bag of words model for each review. This gives a fixed length vector for each review which we can input into a DNN classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GSGA1ZvEwTp"
      },
      "source": [
        "---\r\n",
        "(a) Run the following code and then add comments to explain what is performed in each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuC220KlEni4"
      },
      "source": [
        "# import standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# import functions from keras toolkit\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# load the training data and test data from the IMDB database\n",
        "(Xtrain_seq,ytrain_seq),(Xtest_seq,ytest_seq)=imdb.load_data(num_words=10000)\n",
        "\n",
        "# print out a sample of the data\n",
        "print(Xtrain_seq[:5])\n",
        "print(ytrain_seq[:5])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYyhzqKZMPqa"
      },
      "source": [
        "---\r\n",
        "(b) Here we build a dictionary and a reverse dictionary. Run the code then add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NueQeJgtMP6y"
      },
      "source": [
        "# find out the lengths for each of the reviews\n",
        "listlengths=[]\n",
        "for sequence in Xtrain_seq:\n",
        "  listlengths.append(len(sequence))\n",
        "print(\"Longest list\",max(listlengths))\n",
        "\n",
        "# find out the largest word code used\n",
        "wordcodes=[]\n",
        "for sequence in Xtrain_seq:\n",
        "  wordcodes.append(max(sequence))\n",
        "print(\"Highest code\",max(wordcodes))\n",
        "\n",
        "# get the dictionary from the database\n",
        "dictionary=imdb.get_word_index();\n",
        "\n",
        "# build a reverse dictionary\n",
        "reverse_dictionary={0:\"padding\",1:\"BOS\",2:\"UNK\"}\n",
        "for (key,value) in dictionary.items():\n",
        "  reverse_dictionary[value+3]=key\n",
        "\n",
        "# print out the first 10 items in the reverse dictionary\n",
        "print(list(map(reverse_dictionary.get, range(10))))\n",
        "\n",
        "# convert a review back from numerical codes to text using the reverse dictionary\n",
        "def imdb_review(seq):\n",
        "  review=[]\n",
        "  for widx in seq:\n",
        "    review.append(reverse_dictionary[widx])\n",
        "  return \" \".join(review)\n",
        "\n",
        "# print out the first review\n",
        "print(\"First review:\",imdb_review(Xtrain_seq[0]))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkf1xM0oNQ05"
      },
      "source": [
        "---\r\n",
        "(c) We now vectorise the review documents. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvkaEXUKNhFp"
      },
      "source": [
        "# function to convert lists of word indices into count vectors\n",
        "def vectorise(sequences,numwords=10000):\n",
        "  # output has length of lists but width of dictionary\n",
        "  table=np.zeros((len(sequences),numwords))\n",
        "  # loop over every sequence\n",
        "  for i,seq in enumerate(sequences):\n",
        "    # loop over every word in sequence\n",
        "    for idx in seq:\n",
        "      # add 1 to relevant cell\n",
        "      table[i,idx] = table[i,idx]+1\n",
        "  return(table)\n",
        "\n",
        "# convert training and test data to vectors\n",
        "Xtrain=vectorise(Xtrain_seq)\n",
        "Xtest=vectorise(Xtest_seq)\n",
        "\n",
        "# convert labels to numpy array\n",
        "ytrain=np.asarray(ytrain_seq,dtype='float32')\n",
        "ytest=np.asarray(ytest_seq,dtype='float32')\n",
        "\n",
        "# print out some of the data to see what it looks like\n",
        "print(Xtrain.shape,ytrain.shape,Xtest.shape,ytest.shape)\n",
        "print(Xtrain[:10,:25])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABgvXFckOnjB"
      },
      "source": [
        "---\r\n",
        "(d) Build the DNN model. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvoA4IvbOpen"
      },
      "source": [
        "# use the Keras Sequential model\n",
        "model = Sequential()\n",
        "# add a dense layer of 16 units with 10000 inputs\n",
        "model.add(Dense(16,activation='sigmoid',input_shape=(Xtrain.shape[1],)))\n",
        "# add a second dense layer\n",
        "model.add(Dense(16,activation='sigmoid'))\n",
        "# add a final output layer to give sentiment probability\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "# use cross-entropy to train probability.\n",
        "model.compile(loss='binary_crossentropy',optimizer=\"rmsprop\",metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3qy9MgOPUhw"
      },
      "source": [
        "---\r\n",
        "(e) Train the model. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZoDCCTsPaz4"
      },
      "source": [
        "# train the model, saving history\n",
        "# we use 5% of the training data for validation\n",
        "history=model.fit(Xtrain,ytrain,epochs=20,batch_size=64,validation_split=0.05)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieqTwwWuQ6UX"
      },
      "source": [
        "---\r\n",
        "(f) Plot training graphs. Run the code and add comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlWg9PXNQ8_F"
      },
      "source": [
        "# get the history dictionary\n",
        "hist=history.history\n",
        "\n",
        "# get x axis as number of epochs\n",
        "epochs=range(1,len(hist['loss'])+1)\n",
        "\n",
        "# plot how loss on the training data and validation data changed with epoch\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs,hist['loss'],'b-',label=\"Training loss\")\n",
        "plt.plot(epochs,hist['val_loss'],'r-',label=\"Validation loss\")\n",
        "plt.title(\"Training and Validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# plot how accuracy on the training data and validation data changed with epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs,hist['accuracy'],'b-',label=\"Training accuracy\")\n",
        "plt.plot(epochs,hist['val_accuracy'],'r-',label=\"Validation accuracy\")\n",
        "plt.title(\"Training and Validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMmYJkNaTSHN"
      },
      "source": [
        "---\r\n",
        "(g) Get test performance. Run the code and add comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oiddz_GTTVKD"
      },
      "source": [
        "# evaluate the model on the test data\n",
        "score, acc = model.evaluate(Xtest, ytest,verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "# look at the numerical values of the first 10 predictions\n",
        "ypred=model.predict(Xtest)\n",
        "print(ypred.shape)\n",
        "ypred=ypred.flatten()\n",
        "print(ypred.shape)\n",
        "print(ytest[:10])\n",
        "print(ypred[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwo-IoP4YEz2"
      },
      "source": [
        "---\r\n",
        "(h) Experiment with the problem to try to improve performance. Here are some ideas:\r\n",
        "<ol>\r\n",
        "<li>Change the structure of the network: size, number of layers, node type, optimizer, number of epochs of training.\r\n",
        "<li>Change term counts to term frequencies\r\n",
        "<li>Weight term counts using the TF-IDF method\r\n",
        "</ol>\r\n"
      ]
    }
  ]
}